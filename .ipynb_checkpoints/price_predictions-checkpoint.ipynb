{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noOfDays = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.quandl.com/api/v3/datasets/CHRIS/CME_CL1.csv\"\n",
    "wticl1 = pd.read_csv(url, index_col=0, parse_dates=True)\n",
    "wticl1.sort_index(inplace=True)\n",
    "wticl1_last = wticl1['Last']\n",
    "wticl1['PctCh'] = wticl1.Last.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAFaCAYAAABos5gHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xd4VFX6wPHvGyCRLgqoVKUoEBKD\ngKCoYEdUVETFtlix76o/lV1d26qrYte1V1zbKoJiRQWlqRQpokiR3kREOoQQ8v7+ODO5dyaTZJLM\nTGaS9/M88+TeO/feORPCvHPOec85oqoYY4wxVVVaZRfAGGOMiScLdMYYY6o0C3TGGGOqNAt0xhhj\nqjQLdMYYY6o0C3TGGGOqNAt0xiQREXlNRJJmzE+k8ojIXSKiIrJ/AsvRJ/CaFyXqNU3VYYHOxJyI\n9A18KN0T4bnDAs/tFJE6EZ4fIyIFIjIwcF40j6WBay8K7A8sQ1kbicgdIjJNRDaKSJ6IrBSR90Vk\ngIhIhX4ZSUZEjhKR90RkdeC9/i4in4rI6XF6vdfC/q12B15ztIgcHo/XNCZczcougKmSJgH5wNER\nnusTeC4dOBz4KviEiNQMHPsJmAhcGHbtEOBI4AbgD9/xreUppIgcCnwINAVGA28Cm4HmQD/gfeAa\n4Jny3D/ZiMh9wK3AMuBlYAmwL3AeMEpE/gtcrKq7fZddDlwZg5e/CvfvlAFkBe57kogcp6rjo7h+\nAlAb2BWDsphqxgKdiTlV3Soi04BDRaSOqm73Pd0H+BLICWx/5XuuO1AP+EZV1wJv+O8rIsfhAt0H\nqrq0ImUUkX2Bj4A9gN6qOinslHtE5ESgUSn3qa+qWypSlkQQkUtxQe4r4DT/v4mIDMMFvr8AS4E7\ngs+p6i5iE1xGqGrhlxMRmQiMAG4Big10wd+vqhYAuTEoh6mGrOnSxMvXQC2gV/CAr8Y2HvcNPbzG\n18d3bbzdjKvJDY0Q5ABQ1TGq+k5wP9D09pqIHCsik0RkKy5Ylti3Frwu7NgeIvJQoAlxh4hMFZET\niiusiLQXkf+KyJpAk+PSwPV1S3ujIpIO3IurUZ0X9sUDVc0HrgCWAzeJSBPftfHqMxwT+NnO91pL\nReQbEekSaMLeBPwYeC5iH504l4vIFBHZGnjMEZF/hZ2XISK3isjPIpIbaKb+SES6xOG9mSRjgc7E\nSzBY9fEdC9bYxgce3cM+qPsASgnf8GPoTCAPGF7G67oBHwBTcU2ob5bz9d8GbgKmB35OAkYCXcNP\nFJGugfOOAp7HNad+DPwV+FJEapXyWr1wTZQfquq6SCeoai6uBl0b12wbb+0DP/8IO94KGIdrXr0Z\neKqU+/wXeAH3d3Nf4JpxQGE/beD38zlwJ/Ad7t/tAaATMFlEulXkjZjkZ02XJl4m4wKJv9bWB9iG\n+9DehFfj+8JX2/tRVf+MZ8FEpD7QGpijqjvKeHkmcLyqflXqmcW//gnA6cBwVb3Id3wCMCrCJa8A\na4Du/mZSERmLC47nA6+V8JKdAz9nlFK04PNZpZxXHnsF8nrSgWzgkcDx18POOwC4XFVfKu2GInI2\n7r2/AQwONG8Gn/N/ib8W97fXV1XH+M55Btcf/DChX8hMFWM1OhMXgQAyBejmq7X1ASarar6q/gL8\njvcBE6ztJaLZskHg5+ZyXDu7IkEuIJjh+JD/oKp+AMz3HxORLFxgeAvIEJHGwQeuFrgNKLbJMyD4\nfjeVcl7w+YalnFce84F1wCrgM6AZ8HdVfT7svD+BV6O85/mBnzf5gxxA2P4FwDzgh7DfXzquv/gI\nEaldpndjUorV6Ew8fY1LHjkiUPs4HLjf97y/n65P4Oc3CShXMMDVL8e1C2Lw+m2AgmLu9QtwkG+/\nY+Dn3YFHJPuU8nrB91taAIs2IJbHmYFy7AbWA78EEl3CLQrL+ixJe2BNIHGpJB1xTbIRm20DGgMr\nonxdk2Is0Jl4+hqXwdcH9yEX7J8LGg88JiL1AucU4IJfXKnqFhFZBnQQkdplbL7cXszx4hJRIv0f\nK2lsXvhzwf1HcP1MkWwo4X7gmucADinlvODzc0o5rzwm+LMuS1Dc7zcSoZjfe4Tz5gA3lnBOSUHQ\npDgLdCaevsOlhB+NC3Q7gGm+58fj/gb74PrqZqlqaR/asTISl5RwIS6ZoaL+BBCRvcL6GNtEOHcR\nrrnxQODnsOc6hO0vDPzcXYEm02+BtcBpItI4UsARkT1wTXy5uKbFVDAf9572KaVWtxBoAowLb+I0\n1YP10Zm4UdWduGDXFTgF+E5V83yn/IRrxroZqEtimi2DhuG+xQ8TkcMinSAiJ4jIoCjvF2yGPC7s\n+P9FOPfDwM+bw17vdEKbLQFm4n5PV4pIkaApIjVFZK+SChb4d7gDV6N+I7w/SkRq4AbFtwYeUtXf\nS7pfEglmvA4LSz4hbEab13FZpxFrdCJSWtOvSXFWozPx9jWuRnc4Lr27kKpqYODw6b5zE0JVfxOR\nU3BBZ5KIfIBrNt2MS5ToCxyBm9EjGm8D/wZeEJEOuAB+Eq7vJ/y1x4jIR8DgQJD6HGiLG8v2E16W\nZPB3dCEuZf5HEXkFVwusgxuDNgD4ByVnXaKqL4hIW9wA7bki8jpucPi+wLm4TMs3KL4fMOmo6nsi\n8j/cQPf2IjIa14x7IHAi3u/xCeB44CEROQb3u9yMG8pwLF6rg6miLNCZePMHr0jj48bjAt1u3LRf\nCaOqU0UkE7gOOBX4Fy5p4Xfge9wMIqOjvNdmEekHPIqbgWQrrnn0AiL3oZ2DG8R9Pu5D+Cdcwsa5\n+AJd4N6zAgOb/wH0x03JtQUXqF4DxkZZxqEi8lng/Q4B9sYlnkwH7lTVSEMbkt15uL+bS3G11t24\nqc3eC56gqrtE5GTgalxTdTCYr8aNhyzrWEqTYkQ1aSZKN8YYY2LO+uiMMcZUaRbojDHGVGkW6Iwx\nxlRpFuiMMcZUaRbojDHGVGkpMbygb9+++vnnxc1+ZIwxppoqaTq9QilRo/vjj2imyDPGGGOKSolA\nZ4wxxpSXBTpjjDFVWpUOdPXq1avQ9UuXLuWtt94q0zV33XVXkWPffPMNOTk5ZGZm0rt378LjGzdu\nZODAgXTo0IGOHTvy3XffFbn2ww8/JDs7m5ycHLp168akSZMKn6tRowY5OTnk5OTQv3//Yst0/fXX\nM2FC/Fa/Offcc8nOzuaxxx4r9pwvv/ySrl27kpWVRdeuXRk3blzhcz/88ANZWVm0a9eOv/71rwRn\n63nvvffIzMwkLS2N6dOnF56/a9cuBg8eTFZWFh07duT+++8v8noA8+bN47DDDiMjI4OHH3445LnP\nP/+cgw46iHbt2vHAAw9EvH7ZsmV07dq18N/uueeeK7XMN910U8h7M8YkAVVN+kfXrl21POrWrVuu\n64K+/vprPfnkk6M6d+TIkXrwwQdr06ZN9fDDD9cff/xRVVU3bNigHTt21GXLlqmq6tq1awuv+ctf\n/qIvvviiqqru3LlTN2zYUOS+W7Zs0YKCAlVVnT17th500EGFz0Xz/tavX689evSI6j2Ux5o1a7RV\nq1alnjdjxgxdtWqVqqrOmTNHmzVrVvhc9+7d9dtvv9WCggLt27evfvrpp6qqOnfuXJ03b5727t1b\np02bVnj+m2++qeecc46qqm7btk1bt26tS5YsKfKaa9eu1alTp+qtt96qDz30UOHx/Px8bdOmjS5a\ntEh37typ2dnZ+vPPPxe5fufOnZqbm6uq7t+hdevWhe+huDIvXbpUjz/++FJ/H8aYmIgqhlTpGl0k\nH330ET169KBLly4cd9xxrF3rlrEaP358Ye2oS5cubNmyhb///e9MnDiRnJycEmsrAFdffTXvvvsu\nV111FSNHjqRp06YAvPXWWwwYMIBWrVoBFB7fvHkzEyZM4NJLLwUgPT2dPffcs8h969WrR3DFkW3b\nthG6+kjpRowYQd++fQv3//Wvf9G9e3c6d+7MkCFDCmsiTz75JJ06dSI7O5tBg4quTJObm8vFF19M\nVlYWXbp04euv3VzNJ5xwAr///js5OTlMnFj8nMxdunShWbNmAGRmZpKbm8vOnTtZs2YNmzdv5rDD\nDkNE+Mtf/sIHH3wAQMeOHTnooPBVa0BE2LZtG/n5+ezYsYP09HQaNGhQ5LymTZvSvXt3atWqFXJ8\n6tSptGvXjjZt2pCens6gQYP48MMPi1yfnp5ORkYGADt37qSgwC1lVlKZW7duzfr16/ntt9+K/V0Y\nYxKr2gW6I444gu+//56ZM2cyaNAghg0bBsDDDz/M008/zaxZs5g4cSK1a9fmgQce4Mgjj2TWrFnc\ncMMNrF69mn79+kW8b82aNQuD5j777MM++7glrhYsWMCGDRvo06cPXbt25fXXXwdg8eLFNGnShIsv\nvpguXbpw2WWXsW3btoj3HjVqFB06dODkk0/mlVdeKTyem5tLt27d6NmzZ+EHbbjJkyfTtWvXwv1r\nr72WadOm8dNPP7Fjxw4+/vhjAB544AFmzpzJjz/+GNJEF/T0008DMGfOHN5++20GDx5Mbm4uo0eP\npm3btsyaNYsjjzyS5557LuL1fu+//z5dunQhIyODVatW0aJFi8LnWrRowapVq0q8fuDAgdStW5f9\n9tuPVq1acdNNN7HXXiUuyRZi1apVtGzZMqrXXLFiBdnZ2bRs2ZKhQ4fSrFmzUst8yCGHMHny5KjL\nY4yJr2oX6FauXMmJJ55IVlYWDz30ED//7BZ47tWrFzfeeCNPPvkkGzdupGbNokMMmzVrxqeffhrx\nvu+88w533nknTz/9NEOGDCkcEpGfn88PP/zAJ598wpgxY7jnnntYsGAB+fn5zJgxg6uuuoqZM2dS\nt27dYvuKzjjjDObNm8cHH3zA7bffXnh8+fLlTJ8+nbfeeovrr7+eRYsWFbl2zZo1NGnSpHD/66+/\npkePHmRlZTFu3LjC95+dnc3555/PG2+8EfG9T5o0iQsvvBCADh060Lp1axYsWFDkvCuvvJIrr7wy\n4vsA+Pnnnxk6dCjPP/88QGGN0q+0WuvUqVOpUaMGq1evZsmSJTzyyCMsXry4xGv8yvKaLVu25Mcf\nf+TXX39l+PDhrF27ttTrmzZtyurVq6MujzEmvqpdoLvuuuu49tprmTNnDs8//zy5ubkA/P3vf+el\nl15ix44d9OzZk3nz5pXpvr169WLcuHFccsklNG7cmKFDhwLu237fvn2pW7cujRs35qijjmL27Nm0\naNGCFi1a0KNHD8DVUmbMmFHiaxx11FEsWrSoMIgGmwLbtGlDnz59mDlzZpFrateuXfgec3Nzufrq\nqxkxYgRz5szh8ssvL3zuk08+4ZprruGHH36ga9eu5Ofnh9wn0od7Wa1cuZIzzjiD119/nbZt2wLu\n97Ny5cqQc4LvqzhvvfUWffv2pVatWjRt2pRevXoxffp0nn766cLm55ICTYsWLVixYkWR15wyZUrh\n9aNHhy5D16xZMzIzM5k4cWKpZc7NzaV27ZBFvE1l2rIFAn/npnqqdoFu06ZNNG/eHIDhw731Fhct\nWkRWVhZDhw6lW7duzJs3j/r167Nly5ao7vvTTz8BLrBkZ2cXXnfaaacxceJE8vPz2b59O1OmTKFj\nx47su+++tGzZkvnz5wMwduxYOnXqVOS+v/76a2GQmTFjBnl5eey9995s2LCBnTt3Am5A/eTJkyNe\n37FjR3799VeAwqDWuHFjtm7dyogRIwAoKChgxYoVHH300QwbNoyNGzeydevWkPscddRRvPnmm4Br\njl2+fHnE/rPibNy4kZNPPpn777+fXr16FR7fb7/9qF+/Pt9//z2qyuuvv85pp51W4r1atWrFuHHj\nUFW2bdvG999/T4cOHbjmmmuYNWsWs2bNKjFYdu/enYULF7JkyRLy8vJ455136N+/Pz169Ci8vn//\n/qxcuZIdO3YAsGHDBiZPnsxBBx1UapkXLFhA586di3t5k0jffQdNm0Lz5uD7cmOqmWizVirzUd6s\nSxHR5s2bFz4eeeQR/eCDD/SAAw7QI444Qm+66Sbt3bu3qqpee+21mpmZqdnZ2Tpo0CDNzc3VvLw8\nPeaYYzQ7O1sfffRRXbVqlZ500kkRX+u0007TXr16abNmzbR79+46Z86cwueGDRumHTt21MzMTH3s\nsccKj8+cOVO7du2qWVlZetppp+mff/6pqqrPPvusPvvss6qq+sADD2inTp304IMP1p49e+rEiRNV\nVXXy5MnauXNnzc7O1s6dO+tLL70UsVwTJkzQ888/v3D/tttu07Zt2+qxxx6rF110kd55552al5en\nvXr10s6dO2tmZqbef//9Re6zY8cOHTx4sHbu3FlzcnJ03Lhxqqq6ZMkSzczMLDzPX3a/e+65R+vU\nqaMHH3xw4SOYgTpt2jTNzMzUNm3a6DXXXFOYZTpy5Eht3ry5pqena9OmTfWEE05QVZcBOXDgQO3U\nqZN27NhRhw0bFvG9r1mzRps3b67169fXhg0bavPmzXXTpk2qqvrJJ59o+/bttU2bNnrvvfdGvP6L\nL77QrKwszc7O1qysLH3++ecLnyuuzHl5edqhQwfdtWtXxHuaBLvoIlVwD1/mrakyooohcVthXEQO\nAv7nO9QGuENVHxeR64BrgXzgE1W9paR7devWTf3jqJLZXXfdFXEsXWU64ogj+PjjjyNmdZrYGjVq\nFDNmzOCee+6p7KIYgJ49YcoUt33ddfDkk5VbHhNrUaWhx21SZ1WdD+QAiEgNYBUwSkSOBk4DslV1\np4g0jVcZKkOfPn0quwhFPPLIIyxfvtwCXQLk5+fzf//3f5VdDAOuHvfLL97+smWVVxZTqeJWowt5\nEZETgDtVtZeIvAu8oKpfRXt9KtXojDFJYvVq1zcXlJMDERK2TEpLqtULBgFvB7YPBI4UkSkiMl5E\nuieoDMaY6sRfmwNYvrxyymEqXdwDnYikA/2B9wKHagKNgJ7AzcC7EmEQk4gMEZHpIjJ93bp18S6m\nMaaqCQ90f/4JYdnEpnpIRI3uJGCGqq4N7K8ERgYyZqYCBUDj8ItU9QVV7aaq3fwDno0xJirhgQ6s\nVldNJSLQnYvXbAnwAXAMgIgcCKQDtrKqMSa2Ik36YIGuWoproBOROsDxwEjf4VeANiLyE/AOMFgT\nkRFjjKleItXoLPOyWorb8AIAVd0O7B12LA+4IJ6va4yp5jZtgjVrih63Gl21VKWnAPvtt98YNGgQ\nbdu2pVOnTvTr148FCxbwzTffcMopp1R28YoVacB5cQt9+m3atIlTTz2Vgw8+mMzMTF599dXC55Yv\nX84JJ5xAx44d6dSpE0uXLi1TmZYsWUKPHj1o374955xzDnl5eUXOWbp0KbVr1y6cL9I/ufNtt91G\ny5YtiyyG+9RTT9G5c2f69etXeM9JkyZx4403lql8xoSIVJsDC3TVVbRTqFTmozxTgBUUFGjPnj1D\npqOaOXOmTpgwoUwLqibSli1b9KyzztLGjRtrVlaW3nLLLYXPFbfQp999991XeM3vv/+ujRo10p07\nd6qqau/evfWLL74ofJ1t27aVqWxnnXWWvv3226qqesUVV+gzzzxT5Jzw6cD8vvvuO129enWRxWKz\ns7N19+7deuutt+ro0aO1oKBATzjhhMLp0Iwpl1de8ab+atzY2z7qqMoumYmt6r3w6tdff02tWrVC\nahU5OTkceeSRAGzdupWBAwfSoUMHzj///MIaUnELk/bp04ehQ4dy6KGHcuCBBxYuMrp9+3bOPvts\nsrOzOeecc+jRowfBwe1ffPEFhx12GIcccghnnXVWkYmSw73++uvUrl2bK6+8klmzZhUui1PSQp9+\nIsKWLVtQVbZu3cpee+1FzZo1mTt3Lvn5+Rx//PGAW8y1Tp06ANxxxx1FZuoPp6qMGzeOgQMHAjB4\n8OBi178rTs+ePdlvv/0iPrdr1y62b99OrVq1+O9//0u/fv1o1KhRme5vTAh/jS7wdw9YH101VWUD\n3U8//RSy4Gi4mTNn8vjjjzN37lwWL15cuFBmcQuTgpveaerUqTz++OPcfffdADzzzDM0atSIH3/8\nkdtvv50ffvgBcCsK3HvvvXz11VfMmDGDbt268eijjwLFB5f09HT+/PNP8vLySEtLK5wBP9rFSa+9\n9lp++eUXmjVrRlZWFk888QRpaWksWLCAPffckwEDBtClSxduvvlmdu/eDbjA3r9//xJ/l+vXr2fP\nPfcsXKeupIVKlyxZQpcuXejdu3eJK44H3XTTTfTs2ZN169bRq1cvhg8fztVXX13qdcYU+uMPeOut\n0D45f8blCSd42ytXQuBv31QfVTbQlebQQw+lRYsWpKWlkZOTU9hnVdzCpAADBgwAoGvXroXnT5o0\niUGDBgHQuXNnsrOzAfj++++ZO3cuvXr1Iicnh+HDh7Ms8G2yuODyl7/8hY4dOzJ8+HAOP/zwwmV0\ngrVKv0gLhY4ZM6ZwLbZZs2Zx7bXXsnnzZvLz85k4cSIPP/ww06ZNY/Hixbz22mtR/66iff399tuP\n5cuXM3PmTB599FHOO+88Nm/eXOK9L7zwQmbOnMkbb7zBo48+yl//+lc+++wzBg4cyA033EBBQUHU\n5TTVkCr06wfnnw99+kCw79hfo+vSxS3VAy7IRUpSMVValQ10mZmZhbWrSDIyMgq3a9SoQX5+fokL\nk/qvCZ4PxS9Iqqocf/zxheubzZ07l5dffrnEMqenpzNs2DCuvPJKXn75ZW688UaWLl0a9eKkr776\nKgMGDEBEaNeuHQcccADz5s2jRYsWdOnShTZt2lCzZk1OP/30Uhd5PfHEE8nJyeGyyy6jcePGbNy4\nsfA9F/f6GRkZ7L23S7Lt2rUrbdu2jbgKeSSrV69m2rRpnHbaadx7773873//IyMjg7Fjx0Z1vamm\nfvkFpk1z2wsWwIgRbpHV4IrzInDggdC6tXeNNV9WO1U20B1zzDHs3LmTF198sfDYtGnTGD9+fLHX\nFLcwaUmOOOII3n33XQDmzp3LnDlzANcnNXny5MJFT7dv317qh/7ChQsLMw/bt29Pw4YN2b59e9SL\nk7Zq1aowMKxdu5b58+fTpk0bunfvzoYNGwhOpTZu3LjCRVr/8Y9/MGrUqCL3GjNmDLNmzeKll15C\nRDj66KMLfx/Dhw+P+Prr1q0rbBJdvHgxCxcupE2bNqX8Bp3bb7+9cGmbHTt2ICKkpaWxffv2qK43\n1dQnn4TuP/YYLFwIwZaA/feH2rWhVSvvHMu8rHaqbKATEUaNGsWXX35J27ZtyczM5K677ipx5ek9\n99yTyy+/nKysLE4//XS6dy99vumrr76adevWkZ2dzYMPPkh2djYNGzakSZMmvPbaa5x77rlkZ2fT\ns2dP5gX6DYrro5s3bx59+vTh1VdfpUuXLpx88smFAenZZ5/lsssuo127drRt25aTTjoJgOeee47n\nnnsOcMHi22+/JSsri2OPPZYHH3yQxo0bU6NGDR5++GGOPfZYsrKyUFUuv/xyAObMmcO+++5b6vt8\n8MEHefTRR2nXrh3r16/n0ksvBWD06NHccccdAEyYMIHs7GwOPvhgBg4cyHPPPcdee+0FwC233EKL\nFi3Yvn07LVq0CBlCMTMwo3yXLl0AuPTSS8nKymLGjBn07du31LKZaiw80E2fDv6Wk44d3U8LdNVb\ntOmZlfko7wrjiZCfn687duxQVdVff/1VW7duXZjSX1533nlnDEoWneCq3caknD//VK1Rwxs6EHz4\nj/3f/7lzH3/cO3b11ZVbbhNLUcWQuM6MUh1s376do48+ml27dqGqPPvss6Snp1fonolcvHXMmDEJ\ney1jYuqLL7wMymbN3PpzEJpVGalGZ3101Y4FugqqX78+sV4UNhlXKTcm6fibLYcMgQkTYNy40HOs\n6dJQhfvojDFV2O7d8Omn3v7JJ8P11xc9r0MH99MCXbVmNTpjTOqZOhXWr3fb++4Lhxzittu2hUWL\n3HbTphBIhqJxY5d9uWOHm/B50yZo2DDx5TaVwmp0xpjU42+27NcP0tLc429/844HZhYC3Hg6q9VV\nWxbojDGpxx/oTj7Z277sMje3ZZMmcMstoddYoKu2rOnSGJNaVq6EWbPcdq1aoZM2167tsjFVXS3O\nzz87igW6asVqdMaY1OJPQundG+rXL3pOhLlYbYhB9WWBzhiTWoprtiyNNV1WWxbojDGpIzcXvvrK\n2y9LoLOmy2rLAp0xJnV88w0EJ/o+8EBo3z76a2PZdKkKw4a5sXvBYQ4maVkyijEmdZS32RKgeXPX\nd6fqpgvbtcsls5TH8OEwdKjbrlsX7ruvfPcxCWE1OmNMalCtWKDLyHBzYoJbxueLL8pfjief9PZ/\n+ql89zEJY4HOGJMa5s2DJUvcdv36cOSRZb/HGWd427fcAoHFhMvk++8hsLQUAGvXlv0eJqEs0Blj\nUsPHH3vbJ5wA5Vkl5PbbveEIc+fCq6+W/R5PPx26/9tvZb+HSSgLdMaY1FCRZsugpk29vjVwgW/r\n1uiv//13eO+90GNr17rmTJO0LNAZY5Lfxo0waZK3f9JJ5b/XDTe4xBRwQeqhh6K/9uWXIS8v9Fhu\nLmzeXP7ymLizQGeMSX5jxngLqnbr5lYsKK86dUKzJB9+2PX/FRSUfN3u3fDcc5Gfs+bLpGbDC4wx\nyc/fbHnKKRW/3wUXwGOPwezZblxex44uK7N1a/fYf//QR+vWbmmg4EDzxo3duLwZM9z+2rVw0EEV\nL5eJCwt0xpjktns3fPaZt1/e/jm/GjVcTc4/IfTOnbBggXuU5rLLYP58L9BZjS6pWdOlMSa5TZsG\nf/zhtvfZx1tktaKOO87V6g45xFugNRppaXDllaHNpzbEIKnFrUYnIgcB//MdagPcoaqPB56/CXgI\naKKqf8SrHMaYFOcfVhBcZDVWrr/ePcAllCxb5h5Ll4Y+li3zgu0NN7imzH328e5jNbqkFrdAp6rz\ngRwAEakBrAJGBfZbAscDNrOqMaZkse6fK06DBpCV5R6RbN3qmjf33tvtW40uZSSq6fJYYJGqBmdS\nfQy4BbDBJ8aY4q1aVfwiq4lWr54X5CA00FmNLqklKtANAt4GEJH+wCpVnZ2g1zbGpCr/IqtHHRV5\nkdXKYk2XKSPuWZcikg70B/7y5PigAAAgAElEQVQhInWA24ATorhuCDAEoJV/eQ1jTPXh75+LRbZl\nLFnTZcpIRI3uJGCGqq4F2gIHALNFZCnQApghIkVGf6rqC6raTVW7NWnSJAHFNMYklfBFVuPZP1ce\n/hqdTQOW1BIxju5cAs2WqjoHaBp8IhDsulnWpTGmiPHjvUVW27cv2yKriVC7tktg2bzZrW23YUPZ\nhimYhIlrjS7QVHk8MDKer2OMqYJiMYlzvFk/XUqIa6BT1e2qureqbirm+f2tNmeMKUI1ufvngizz\nMiXYzCjGmOTjX2S1Xj2XcZmMLCElJVigM8YkH3+zZXkXWU0Ea7pMCRbojDHJJxWaLcFqdCnCAp0x\nJrls3x66yGq/fpVXltJYjS4lWKAzxiSXX3/1Fllt165ii6zGmyWjpAQLdMaY5PLrr952so2dC2dN\nlynBAp0xJrksWuRtt21beeWIhjVdpgQLdMaY5JJKga5pU2973TqvydUkFQt0xpjk4m+6bNeu8soR\njYwMaNTIbe/eDevXV255TEQW6IwxySWVanRgCSkpwAKdMSZ55OXB8uVuWwQOOKByyxMNS0hJehbo\njDHJY+lSKChw2y1awB57VGpxomIJKUnPAp0xJnmkUv9ckNXokp4FOmNM8ki1/jmwGl0KsEBnjEke\nqRjoLBkl6VmgM8YkD2u6NHFggc4YkzxSsUZnTZdJzwKdMSY57N4Nixd7+6kS6KxGl/Qs0BljksOq\nVW4cHUCTJtCgQeWWJ1pNmrgxfwB//AG7dlVueUwRFuiMMcnB3z+XKrU5gJo1oXFjt63q5rw0ScUC\nnTEmOfj751IlESXImi+TmgU6Y0xySMVElCB/QsqaNZVXDhORBTpjTHJIxaEFQc2aedurVlVeOUxE\nFuiMMckhlWt0rVp52ytWVF45TEQW6IwxlU81dZNRAFq29LaDqy+YpGGBzhhT+datg61b3Xb9+i5l\nP5X4A53V6JKOBTpjTOULr80Fx6WlCmu6TGo1i3tCROYAGukpQFU1O26lMsakvt9/h3fegaOPhqys\nks9N5aEFULRGp5p6wboKKzbQAackrBTGmKrnvPNg7FioVQtGjoRTSvhISeVEFHCzuDRoAJs3Q26u\nmyEl1Zpfq7Bimy5VdVnwETjUPrD9O/BnQkpnjElN8+e7IAduSqwzz4RPPy3+/J9+8rZTsUYHoc2X\nsUpIUYXXX4dXXoH8/NjcsxoqtY9ORC4HRgDPBw61AD6I4rqDRGSW77FZRK4XkYdEZJ6I/Cgio0Rk\nz4q9BWNM0nnjjdD9vDw44wz4/POix6+6Ct5/3zt24IHxL188xCMh5aWXYPBguPRS+N//YnPPaiia\nZJRrgF7AZgBVXQg0Le0iVZ2vqjmqmgN0BbYDo4Avgc6BPr4FwD/KWXZjTDJSDQ109eq5n3l5cPrp\ncN117vlp01z/3XPPeef26gVHHJHY8sZKrGt0qvDII97+tGkVv2c1FU2g26mqecEdEalJ5CSVkhwL\nLAo0hX6hqsE6+Pe4GqIxpqqYPBmWLnXbjRrBDz9A69Zuf+dO+M9/4MIL4dBD4dtvvevOOQfGjIG0\nFE0Gj3WNbuxY1wQcZHNolls0f1HjReRWoLaIHA+8B3xUxtcZBLwd4fglwGdlvJcxJpn5a3Nnn+2a\nIr/5pvi+t7Q0eOghePttqFs3IUWMi1gHuqefDt23QFduJWVdBv0duBSYA1wBfAq8FO0LiEg60J+w\nJkoRuQ3IB94s5rohwBCAVv4mAWNM8tq5E95919u/4AL3c//9XcLJuHEwdSpMmeKa4urVgxdfhOOO\nq5TixlQsmy6XL4fRo0OPWaArtxIDnYjUAIar6gXAi+V8jZOAGapa+K8kIoNxwxeOVdWIzaCq+gLw\nAkC3bt3K2lRqjKkMn3wCGza47QMOcH1uQRkZcNJJ7lEVxbJG9/zzUFAQeuz33yt2z2qsxKZLVd0N\nNAnUysrrXHzNliLSFxgK9FfV7RW4rzEm2fibLS+4oHoNmm7hSzdYvbr8wwF27nS13HDr19sQg3KK\npulyKTBZREYD24IHVfXR0i4UkTrA8bgmz6D/ABnAl+L+E3yvqleWoczGmGT055/w8cfefrDZsrrI\nyHDr0q1d62pjq1eHNmdGa8QIb5XyFi1g+3b3uw2uXr7ffrEtdzUQTaBbHXikAfXLcvNAjW3vsGMp\nOhrUGFOid991g8PBZVSm6ni4imjZ0utLW7Gi7IFu0aLQIQVXXumSdP4MzNGxdq0FunKIpo+unqre\nnKDyGGNSlb/Z8sILK68clalVK5g+3W0vXx7aR1mcTZvgvfdg+HCYNMk7XqsWXHaZS+D5+Wd3zPrp\nyqXEQKequ0XkkEQVxhiTohYvduPnAGrWdGPiqqNoE1J273bj5IYPd/OA5uYWPeeyy1xT6D77eMcs\n87Jcomm6nBXon3uP0D66kXErlTEmtbzpGyXUt2/1ndC4tCEGv/zigtsbb8CqVUWfr1ED+vWDiy5y\ns8gANPVNRGWBrlyiCXR7AeuBY3zHFLBAZ4xxSRL//a+3X92SUPyKq9H973/w6KNuDGEkBx/s5rQ8\n77zQGhxYjS4GSg10qnpxIgpijEkS27e7R+PG0Z0/dSosXOi2GzSA/v3jV7ZkFynQTZ0KgwYVPbdJ\nE/elYPBgF+iK4w901kdXLqUGOhHZAzczSiawR/C4ql4Sx3IZYyrD779DZqYbszVqFJx2WunX+JNQ\nBg6E2rXjV75kF6np8tVXvWPp6XDqqS649e3rEk5KYzW6Cotmrsv/AvsCJwLjcZMwb4lnoYwxleSj\nj9yioaquqa00u3a5VcSDqnOzJbigVDNQf1i/HjZuDF1eZ8wYN07u1FOjC3JgfXQxEE2ga6eqtwPb\nVHU4cDKQFd9iGWMqhT+B4ttvXep7ST7/3AVGcM12vXvHr2ypoEYNaN7c23/+eW9KtNat4aijyn5P\nq9FVWDSBLjAClI0i0hloCOwftxIZYyrPypXedn6+G8NVEn+z5fnnp+4SO7Hkb7587DFv+4ILyvf7\n8dfo1q0rOgemKVU0v/UXRKQRcDswGpgLPBjXUhljKkf42K/wFcH9Nm2CDz/09qt7s2WQPyHFXwMr\n7+9njz1ckg+4Lx/BGqKJWjRZl8ElecYDbeJbHGNMpfLX6MAFOtXIkzO//76bgBigSxeXxGIiT/vV\nrRt06FD+e+6zD2ze7LbXroW99y75fBOi1BqdiOwtIk+JyAwR+UFEHhcR+y0bU9WoFq3RLV8O8+ZF\nPt/GzkXmr9EFVXRKNBtiUCHRNF2+A/wOnAkMBP4A/lfiFcaY1LN5M2zdWvT4mDFFjy1f7lYNB9fv\ndO65cS1aSgkPdDVqRB5HVxaWkFIh0QS6vVT1HlVdEnjcC+wZ74IZYxKsuLkZI/XTvfWWt3388Taj\nvl940+WJJ4YmlJSHDTGokGgC3dciMkhE0gKPs4FP4l0wY0yC+fvn2rf3tsePhx07vH2b8qtk4TW6\nWKzkYDW6Cik20InIFhHZjFs09S1gZ+DxDnBDYopnjEkYf43u8MOhUye3nZvrgl3QzJkwd67brlsX\nzjgjcWVMBY0awQEHuO29947NlGjWR1chxQY6Va2vqg0CP9NUtVbgkaaqDRJZSGNMAvhrdC1auCmq\ngvzNl/6xcwMGuGBnPCJu9pO//Q0+/RTq1Kn4Pa3pskKiWb3AGFMd+Gt0LVu6WklwGrBgoMvPD+2f\ns2bLyA45xD1ixZouK8QCnTHGCa/RHXWUm6B5xw6YPx+uucYlWgQ/aPfdF449tnLKWt1YoKsQC3TG\nGCe8RrfHHtCnD3z2mTv2zDOh5593nkudN/EX3kdX3CB+E1GJWZeBLMufElUYY0wlCR8s3qKF+3nH\nHa7mFkkssglNdOrVc188wNWwI413NMUqMdCpagEwW0QizGljjKkyNm2Cbdvcdp06LnMQoGdPFwDH\njoVrr/Vm5j///JIXCzWxJWLNlxUQTdPlfsDPIjIV2BY8qKrVeBlhY6qY8Nqcv1msZk045hj3eOIJ\nFxDr1bOms0TbZx9Ytsxtr10L7dpVbnlSSDSB7u64l8IYU7n8iSiR5moMSkuD+vXjXx5TlH+IgY2l\nK5NoVi8YLyKtgfaq+pWI1AGsB9qYqiRS/5xJLtZ0WW7RrF5wOTACeD5wqDnwQTwLZYxJsGhrdKby\nWKArt2jmurwG6AVsBlDVhUAFZyg1xiQVq9ElPwt05RZNoNupqnnBHRGpCWj8imSMKUIV7rvPzSsZ\nnGcylqxGl/ysj67coklGGS8itwK1ReR44Grgo/gWyxgT4oMP4J//dNuqbj+WrEaX/KxGV27R1Oj+\nDqwD5uBWMvgU+Gc8C2WM8VGFe+/19n+K8RwOqlajSwUW6MotmqzLAhEZDkzBNVnOV1VrujQmUcaM\ngRkzvP3ly2H37thNv7VxY+hg8T1tXeWkZCsYlFs0WZcnA4uAJ4H/AL+KyElRXHeQiMzyPTaLyPUi\nspeIfCkiCwM/G1X8bcTR99+7yWy//76yS2Kqo/DaHMCuXbB6dexeI7w2ZwPBk9Nee3lfbjZvdusE\nmqhE03T5CHC0qvZR1d7A0cBjpV2kqvNVNUdVc4CuwHZgFK4pdKyqtgfGBvaT0x9/wIknuslszzzT\nfeiY1LRzJ1x8sZuR/7vvKrs00ZswASZPLnp86dLYvUb4ZM4mOaWlWUJKOUUT6H5X1V99+4uBsv6G\njwUWqeoy4DRgeOD4cOD0Mt4rce67z31zAvcNet26yi2PKb9HH4XXXoOJE91UVqNHV3aJohNemwta\nsiR2rxG+PI9JXv5+ujVrKq8cKSaaQPeziHwqIheJyGBcxuU0ERkgIgOifJ1BwNuB7X1UdQ1A4Gdy\njslbsgSefjr0mP+br0kd69fDgw96+7m5Lk3/5Zcrr0zRmDIFvvrKbaelwcCB3nNWo6ueWrf2tmP5\nN1DFRRPo9gDWAr2BPrgMzL2AU4FTSrtYRNKB/sB7ZSmYiAwRkekiMn1dZdSk7rjD9YX4+b/5mtRx\n//1udn6/ggK47DJXYyooqJxyleb++73tc88NXeQ0lh9yVqNLHW3aeNuLFlVeOVJMNFmXF1fwNU4C\nZqhqME1orYjsp6prRGQ/imkGVdUXgBcAunXrltjOsdmz4c03ix63Gl3qWb4cnnrK23/6aXjpJZg5\n0+3ffjt88w288opbPTtZqMIXX3j7//hH6N9fLJsurUaXOvyBbvHiyitHiommRldR5+I1WwKMBgYH\ntgcDHyagDGXzj394iSf+FG4LdKnnzjshLzCxT48ecNVVLrD5a0djx0JWlgt2yZJwtG2bW2AT3IKb\nnTrBAQd4z1uNrnpq29bbthpd1OIa6AIrHRwPjPQdfgA4XkQWBp57IJ5lKLNvvoHPPnPbInDDDd5z\nFuhSy5w5MHy4t//gg+7ftEED+PRTuOUW1/cFLuno0kvh1FOTo5P/jz+87caNXbn9Nc4VKyA/v+Kv\nE76yuNXokpvV6MolroFOVber6t6qusl3bL2qHquq7QM//4xnGcpEFYYO9fYHD4a+fb19C3Sp5bbb\nvBraSSdB797ec+npLvBNnBi6gOUnn0BmJrz9duXW7sIDHUDt2rDvvm579+7Y9BmvWwfbt7vtunWh\nYcOK39PEz/77e+McV6zwWitMiUoMdIFB34+IyCeBx8MiclCiCpdwI0fC1KluOyMD7r47tCnHAl3q\n2LQJPv7YbYuEJnb4HX6465P961+9Yxs2wHnnwdlnV96QEn+ga9LE24518+WsWd525842WDzZZWR4\nn0mqlnkZpWIDnYgcBnwDbMElhbwIbAO+FpGeCSldLBUUuMSE4uTnw623evvXXuuaivxNOatWJW+G\nngn1yy9ejSwzEw4+uPhz69SBJ56AceNC07dHjHDXjhoV37JG4g+wwRoduG/0QbH4kAsm5QDk5FT8\nfib+rPmyzEqq0d0BnKuqd6nqh6r6gareiUsuuTMxxYuBzZvhrrtcJ27v3sUHqldegQUL3HbDhi4h\nBdyH4F57ue38fJtjLlX88ou33bFjdNccfbTr17v8cu/YunUwYABceKGr6SVKpKZLCA10sci89Nfo\nunSp+P1M/FlCSpmVFOjaquo34QdVdTzQpujpSSo9HZ580n37XbrUTakUbvt2FwyDhg6Fvff29v21\nOmu+TA3lCXQA9evDCy+4hKRmzbzjb7zhmvY+/zx2ZSxJcYEu1k2X/hqdBbrUYDW6Misp0G0p4blt\nsS5I3OyxhxtsG/Taa0XPeeIJL9Nuv/3gb38Lfd4CXeopb6AL6tvXLYdz4YXesdWrXVLL5Zd7U8PF\nS3F9dLFsuty2zWvFSEtzgdwkP3+NzgJdVEoKdC1F5MkIj6eA5okqYExc7Bvz/t57sMUXw9evhwd8\nIxzuuss1V/pZoEs9/kDXqVP57tGoEbz+uuuj80+m+9JLkJ0debLlWElE0+WPP3r9mB06FP27N8mp\npNlRvvrKTZAwfjxs3ZrYciWxkmZGubmE56bHuiBx1bWrSyr4+WfXTDlihBf8/v1v79v5gQfCJZcU\nvd4yL1PLjh1eEEhLc/+uFXH66dCrF1x9tfvbAVi2DPr3h4ULvT7cWCouGaVVK5cZqeqSo/LyXPN8\neVizZWoKb7pUdX8TkybBCSd4X17S0tzn3qGHuskSDj3U7dcsdUKsKqfYGp2qDg9/4GY1eT2wnTpE\n4KKLvP1g8+WyZfCf/3jH778/8h+Bv0Zn810mvwULvKSjAw5wzdcV1aQJvPuuG1/XKLCE4p9/epMu\nx1pxNbqMDK/vsKCgYn+P/kQUy7hMHXvv7SY9ANf8HFyu5513Qsd+FhS45KqXX4YhQ9y/ccOGbqmq\nm292rVuxXNcwiZU0vOAOEekQ2M4QkXG4BVjXishxiSpgzFxwgTed14QJrsofPj3UGWdEvtaaLlNL\nRfvniiMCgwbBddd5x8aNi939/Yrro4PYNV9ajS41iUROSBk71jvmH1jut327myTh4YfdONH99/da\nKaqwkvrozgHmB7YHAwI0wa1i8O84lyv29t3XJRIEDR3q+l+CgtNDRWKBLrXEK9AFHXOMtx2PQFdQ\n4PqOg/wZwBCbhJRdu9y3/SCr0aWW8ISUVatg3jy3n5EBc+e6SRO+/tp9tg0YEHke01274NVXE1Pm\nSlRSY22eamE9+ETgHVXdDfwiIqnZyHvRRd5sGe+/7x0Pnx4qnP8PZPVqN56uGrZzp4xYJKKUpGdP\n1xyam+v66FasiO0ckRs2eE2vDRtCrVqhz8diiMH8+W7VdXBlDw+mJrmFJ6T4xwcffribLg6gTx/3\nCFq92s3+9PHH3nqMfybPLIzxUlKNbqeIdBaRJsDRgG/NEFIzPeuUU4omDpQ0PVRQRoaXdVdQEP9J\nf3/9FY44wn1Iz59f+vkmVLxrdBkZ7t8n6OuvY3v/4vrngmLRdGnNlqktvOnS37LgX5kjXLNmLrnK\nP4QqfK3GKqikQHc9MAKYBzymqksARKQfMLOE65JXRgacf37osQsuKHl6qKBoMy8rOhHwr7+6b2CT\nJ7sP7Geeqdj9qpv8fG9sGLi0+XiIZ/NlSf1zEJumS0tESW3hs6P4++dKCnRB/sm7q3OgU9XvVbVD\nYPWBe3zHP1XVc4u7Lun5sy/T0+Ff/4ruumgyL596CurVc69RnoC3eLGbhmrVqtJfy0S2eLGXYNSs\nWfxm4w8PdLFc6aC0Gl2kpssPP4Qjj3RLDX37benlsRpdavPX6KZN8758N2gA3bqVfv2ee3rb1TnQ\niciNYY8bRORCETmguGtSQpcu8Pe/u/FIL7wQ+u24JKUlpKi61Q62b3droPkTXaKxZIkLcuGBrbJm\nz08F27bBffe5ybiDi5TGu9kyqGtXN10YuL+HWM45WFqga9HCW0dv9Wp45BGXMTxpkpuztVcvN17q\nkUci//2o2mTOqa5VKy+LPNjXCi7XIJr8gXr1vOS7bdtcUkoVVlLTZf2wRwOgG/CZiAxKQNniI9gn\nt2yZW28uWqUFut9+C82Uu/nm0icBVnULvV5wgeuPC66u4M/+tEAX2fTpcMgh8M9/un/P4MoTiQp0\nNWuGJjDFsvmyuMHiQenp0DwwOZEq3HRT0RrcL7+4482bw1lnuTk6d+92zy1fDhs3uu1GjUJXbDCp\noVat0IV4g6JptgT3RSk4Fg/iP6VdJSup6fLuCI+/AYcDtySuiEmitEA3d27o/rp1buHPSNascR/O\nBx7oanFvvuky+MD1I/pXxbZAF2r3bve7O+yw0L645593K0vEO+PSL179dKXV6CC0+TKoZ083D2e9\net6xXbvcOKmTTnLX3HUXfPSR93xOjq1Bl6r8/XRB0QY6qFb9dGVeYTywInj1+59R1kAH8Nxzrv0c\nXJLERx/Baae5e916q0s88evSBb74wg1KDvrzT++beHW3bJkLLrfe6n6ffjt2wGOPJa5GB+5LSlAs\n++lKS0aBok3uJ5/sEhJeeMF9kXr5ZfdlwG/FCte87h/wbs2WqcvfTwewzz6uyTpa1aifrsyBTkSO\nARK4MFeSKC3r0h/oMjLcT1W48kr3wdyqlZsbcfTo0MDVsCFccw3MmOEeRx3lmiWC00yphjaJVldv\nv+2yY/3LLPXsCY8+6u0//bSbzzQo3oEuO9sbrrJuXehrV0Q0NTp/bfKii9zE08FJmevVc3O2fvut\nK9ONNxZ/H0tESV3hge6YY8pWO/fX6IJN2VVUSckoc0Tkx7DHSuAB4OrEFTFJNG/u/RGtXetl9gX5\nP+QeftgLdjNmuKa28LF3vXvDf//rjv/nP0U/cPzf5P0ffNXNpk2uD/O887xvnWlprglu4kQ3Hij4\nLXbrVpcMBO6Lgn/FgXhISytaq4uF0vrowP1O3n3XtRK88krRQeVBnTq5pJSVK935J57o/R3XqhU6\nmNiklvCmS/+Xn2hUo6bLktJzTgnbV2C9qqbOWnSxVKuWm0ZszRpXy1q92ms+Ug0NdKee6oLT3XeH\n3mPffd2370sugfbtS369Jk28Pqjq2k83aZL7QF+2zDvWpo1bBNXfLHfrrUXHR3bsmJi+p2OO8WbZ\nGTsW/vrXit8zmhpdjRouySRaGRnu/LPOcr/Pzz93zZaxnNHFJFZ4ja4s/XNQrQJdSckoy8Iey6tt\nkAsqrp9u3TpvGp26dd15Q4e6b8+1a7sZWT74wGW73X9/6UEOQj/gqlug27ULbr/d1Xr9Qe6ii9xA\n5/C+p7PPhnbtQo/FOxElyP8tesKE2PSnRtNHVxGtW8MVV7iJzE3qOuggL1hlZUVOUCpJNeqjswkb\ny6JlSzdPHIQGOn9trlMn16RVu7b71lxe/g+46hTotmyB44+HKVO8Y3vu6ZIsiqvB1KzpxkZedpl3\nLN79c0EHHeRq6r/95vo5fvyxYv1eeXleqneNGvEb8G5SX9267gv0hx+6bNuyshqdiai4Gp0/ESVW\nNYnqGuiGDQsNcn36uOBRWjPdhReG/vt07hyX4hUhEtrPVdF5L8NXLUiz/6KmBH36uGzj8nzuWDKK\nicifeemfwcQCXez4x3jddpvr94qmHyk9HV580aVY9+1b9o75ivAHum++qdi9oklEMSYWqlGNzpou\ny8I/E4E/uFmgi43Vq2H2bLddq5br5yxLjebEE10TYqL5My+D/XTB6ZnKKt79c8YEVaM+OqvRlUXP\nnt72+PFeM5O/j64sAzZLUh2HF3zhWwmqVy9vLslk17497Lef2960yQvW5RFNxqUxsVCNanQW6Mqi\nZUsvU233bjf4e906r8ZVu3bs5g2sjlmXY8Z42337Vl45yiq8n64izZcW6EyiWB+dKZY/KeK994pO\nORWr5IHq1nS5e3dojS6VAh3ELtBZH51JFKvRmWKdeaa3/dVXboHUoFiO3QpvuozlemfJaPp0byzi\nvvu66bVSiT/QVWQ8ndXoTKJYH11siMieIjJCROaJyC8icpiI5IjI9yIyS0Smi8ih8SxDzO2/P3Tv\n7rZ37YInnvCei1X/HLhm0Lp1vdepSn+IU6bAX/4Cn3ziHfOPOezbN/Vm1G/f3i30Cu7fyr+Cd1lY\nMopJFKvRxcwTwOeq2gE4GPgFGAbcrao5wB2B/dQycKC3vXattx3r2TiqYvOlKpx7rpvns39/l9QD\nRQNdqolVP53V6Eyi1KnjZQfn5oYu4FrFxC3QiUgD4CjgZQBVzVPVjbg5M4Mr/jUEVserDHHjD3R+\n8Qx0VSXzcuFCt5o6QEGBm8ty4UJvxpm0NDjuuMorX0VYoDOpRKTa1OriWaNrA6wDXhWRmSLykojU\nBa4HHhKRFcDDwD/iWIb4aNPGrW7tt8ceZZ9rrjRVsUY3cWLo/sqVbjLaggK3f+ihbkaQVBSLfjpL\nRjGJVE366eIZ6GoChwDPqmoXYBvwd+Aq4AZVbQncQKDGF05EhgT68KavS8YP+fApqTp0KP8g4eJU\nxSEG4YEOQqdTS8Vmy6B27dxyTuDmqxw1yi0dFC1V66MziWU1ugpbCaxU1eDEhSNwgW8wMDJw7D0g\nYjKKqr6gqt1UtVuTZPwPH958GY/Z8qt6je6II4o+f+KJiStLrIX30511lhv03qqVW4C3tFlbtm3z\n+klq1/YWUjUmXizQVYyq/gasEJGDAoeOBebi+uR6B44dAyyMVxniql07t55XkAW60q1eDYsXu+09\n9oCPPw6dfLlRIy+jNVWdemrRYytWwPPPw4EHukV5wxftDbL+OZNo1WTQeLyzLq8D3hSRH4Ec4N/A\n5cAjIjI7sD8kzmWIn2uvdT9FIn/AVVS0ge7FF+Gmm1xSRzLz1+Z69HD/yd55BxoEcpMuvzz2zb+J\ndtZZ8MwzcMYZrjm7pm862S1b4Oab3dphn31W9FrrnzOJVk366OI6qbOqzgK6hR2eBHSN5+smzCWX\nuPXIGjWK7Ri6oGgC3fffw5DAd4WnnnIfpLfempzNXv5Ad+SR7mdmppsUe8GCyE2ZqSYtDa66yj3A\njYEcOxZuvNGbRWfBAq8yx4gAACAASURBVOjXzy3I+9hj3qKx1j9nEs2aLk2pRNyHczyCHEQ3vMAf\nPPLy4L77XDPqBx8k32wqkQIduASOo492KxZUNbVquQSb2bNdUAvWXsE13WZmukVjt2yxpkuTeOUN\ndKqQnx/78sSJBbpkFk3W5Y8/Fj22bJlrOjvlFFi0KD5lK6uNG2HOHLedlha6EkR1UKsWXH+9a16+\n9FJv5pe8PHjwQdcy8M473vkW6EwilKePbtEiNwvQAQdE/vxJQhboklk0TZf+P7Trrgv9gPz0U1dj\nuPNO2LEjPmWM1uTJXg0zJye0ZlOdNG0KL73kBsgfdph3fM0a9+8VZIHOJEJ5+ugeeshlEK9cCRdd\nVP55XRPIAl0ya9DAa87bvt09/HbtCl094Z57YP58l8oerDHs3An/+pcLeB9/nJhyR1Jcs2V11a0b\nTJoEr7/uJrEOZ4HOJEJZmy5VQ7+QzZwJzz4b+3LFmAW6ZCZScq1u/nwX7MCtg9ewIey1l/vDmzo1\nNFV/yRKXGdq/v5fin0gW6IpKS4MLL3TJKUOHhvZR+oddGBMvZQ10P/8cOsEDwD//WfoY0UpmgS7Z\nlRTo/M2WWVmhz3Xr5jIyX3jBBb+gjz5yySojR5IwO3bAtGneflXIroyl+vXhgQfch8hNN8GTT9rv\nyCRGWfvo/LW5oE2bXLZ3ErNAl+yiDXSR1m9LS3Nj0+bPdz/9zZl33x37shZn6lSv5nnggbDPPol7\n7VTSvr3r/7juutRbpsikprL20fkD3QUXeNtvvOGtRJKELNAlu5KGGJQW6IIaN3Y1u+++8wZkz5nj\nLXQab9ZsaUxyKkvT5aZNrl856KGH4Oyzvf2rr/a+0CYZC3TJrqQhBtEGuqAePaBrYKy+aujq6PHk\nX7LGAp0xySM80JU09vbLL70My27dXBLVo49CvXru2Ny58Pjj8StrBVigS3bFNV2uXw+rVrntjAzX\n7BWNo47ytidMqHj5SpObGxpQjzkm/q9pjInOHntAerrb3rXL/X8tziefeNv9+rmfzZuHdoPcfXfR\nZJUkYIEu2RUX6IKDr8ENHagZ5Wxu/hpVIgLdt996/3nat4eWLeP/msaY6Pn76YpLSCkoCJ2fNRjo\nwPUpB7OEt22DG26IfRkryAJdsisu0JW12TLIn803Y0bZ1ksrj3HjvO1jj43vaxljyi6afrqZM2Ht\nWrfduLFrugyqVctNZB70/vswZkzsy1kBFuiSXTQ1urIEur328oYi5Oe7IQjxNHast22BzpjkE02g\n82db9u1bdJWRI4+EwYO9/WuvLbkZNMEs0CW74rIuy1ujg8T1023e7I2fE3ETNxtjkktZA52/2dJv\n2DCvGfTXX91+krBAl+wi1eh274affvKOJ2ugGz/ey9LKyYG9947faxljyqe0Pro//oApU9x2Whqc\neGLk+zRt6lZPCbr//sqZhSkCC3TJrlEjb/Dwxo0uM2rxYm/ey332KfvaZf6ElClT3ADyeLBmS2OS\nX2k1utGjvWEHhx0WOtNSuCuu8IYw5ea6RJUkWC7MAl2yq1EjtCb0xx8Va7YE2G8/b7HP3FyYPr1i\nZSyOP9DZsAJjklNpge79973tM84o+V41ari5doNfzj/9FD78sOJlrCALdKnAX2Nbs6bigQ7i33y5\ndq3XvFqzpg0UNyZZlRToNm1yA8WDBgwo/X7du8OQId7+3/7mhh1UIgt0qcAf6Pr0gZdf9vaTNdD5\nhxX07OnNnmCMSS4l9dF9/LE3rdchh7jFVqPx7397szotXw733lvxclaABbpU4K8NbdnizYgC5Q90\n/ntOnhz7xRNt/JwxqaGkGp2/2XLgwOjvuddeoVmXjzwSunZmglmgSwV33+0GZHboEHo8PR06dizf\nPQ84wE3fAy54zp5dsTKGs0QUY1JDcYFu2zb4/HNv/8wzy3bfwYPh8MPd9q5dbgmqSmKBLhXUqAFX\nXeUmTf3iC7eAapMmrjkgI6N89xQJbb70z0peUQsWuIVeAerUcZNJG2OSU3GB7rPP3FqS4Kb4OvDA\nst03Lc19Qfcnpnz1VcXKWk4W6FKJCBx/vEv3/f33ii922KWLtx0MTBW1Y0foOlW9e3uTxhpjkk9x\nfXT+Zsuy1uaCDj4YLr7Y27/ppth3k0TBAl111qKFt71yZcXvpwqXXurNhlKjBgwdWvH7GmPiJ1KN\nLjfXJaIElTfQAfzrX1C7ttuePdst0ppgFuiqs1gHuvvvh7ff9vYfe8zV6IwxyStSoPvyS2/C9/bt\nvdUJyqN589D+uX/+05vwIkEs0FVnwWQUCM3kLI9Ro+C227z9IUPcxK7GmOTmD3SbN7uWmfBmy2A/\nW3ndfLObxQncl+oEL9Bqga46a9bM2169uvxt57Nnw4UXevt9+sB//lPx/xzGmPirVcsljYH7DJgx\nI7RlpiLNlkH164cu0PrAAy7PIEEs0FVne+zhDUbfvdtbb6os1q51WaDBmQ/atIERI9x/HmNMavDX\n6gYPhrw8t92jhzd3ZUVdeqk3HGrLltDAF2cW6Kq7ivTT7dzppgRascLtN2gAH31kqxQYk2r8ge7n\nn93PGjXg+edj1zJTsyY89JDb7t/fTficIBboqrvyBjpV1w/37bduPy0N3nkHOnWKbfmMMfHnD3RB\nN97ohgfEUr9+8MMPbqLn8Akw4iiugU5E9hSRESIyT0R+EZHDAsevE5H5IvKziCTP6nzVUXkD3cMP\nw+uve/sPPQQnnRS7chljEsc/lg6gdWu4887Yv46ImzMzwWrG+f5PAJ+r6kARSQfqiMjRwGlAtqru\nFJGmcS6DKUl5Mi8//jh0fNwll8ANN8S2XMaYxAmv0T37LNStWzlliYO4BToRaQAcBVwEoKp5QJ6I\nXAU8oKo7A8cTl3pjiiprje6nn+Dcc73FFI84InSaH2NM6gmm/gOcfXaVa52JZ9NlG2Ad8KqIzBSR\nl0SkLnAgcKSITBGR8SLSPY5lMKUpS6Bbt85lWAYHku6/P4wcWf75No0xyeGKK1xG5JFHwlNPVXZp\nYi6eTZc1gUOA61R1iog8Afw9cLwR0BPoDrwrIm1UQ9dbF5EhwBCAVq1axbGY1Vy0gW7XLrdMx9Kl\nbr9ePTfnpn+tPGNMasrMdJPGV1HxrNGtBFaq6pTA/ghc4FsJjFRnKlAANA6/WFVfUNVuqtqtiX2Y\nxo+/j27lSq9JMtzLL3sLtIrAW29BVlb8y2eMMRUUt0Cnqr8BK0TkoMChY4G5wAfAMQAiciCQDvwR\nr3KYUtSr52Vc5eXBHxH+KVRdP1zQnXe6JkxjjEkB8c66vA54M5BxuRi4GNgGvCIiPwF5wODwZkuT\nYM2be8tzrFxZtDny229hzhy3XacOXH99YstnjDEVENdAp6qzgG4RnrogwjFTWVq08GZDWLUqdJ06\ncKnGQeefH3lwqTHGJCmbGcWUnJCybh289563f+WViSmTMcbEiAU6U3Kge/VVb4LXQw+tlFkNjDGm\nIizQmeIDXUGBm9Q16KqrElcmY4yJEQt0pvhA9+WXsHix295zTzdjgjHGpBgLdKb4QOdPQrnoIm9x\nRmOMSSHxHl5gUkGkQeNr17q15YKuuCLx5TLGmBiwGp1xzZLB2tq2bbB5M7z/vuujAzf/XQLXjjLG\nmFiyQGfclF7hzZfvvuvtDxqU+DIZY0yMWKAzjj/QTZ0KEye67bQ0GDCgcspkjDExYIHOOP5A98QT\n3uTOvXvDvvtWTpmMMSYGLNAZxx/oZs/2tm1IgTEmxVmgM44/8zLImi2NMVWABTrj+Gt0QUcfDU2b\nJr4sxhgTQxbojBMp0FmzpTGmCrBAZ5zwQFejBpxxRuWUxRhjYsgCnXEaN4b0dG//mGOKLsBqjDEp\nyAKdcdLSQhNSrNnSGFNFWKAznr593c/99oMzz6zcshhjTIzYpM7G8+STcPrpkJkJjRpVdmmMMSYm\nLNAZT82acMIJlV0KY4yJKWu6NMYYU6VZoDPGGFOlWaAzxhhTpVmgM8YYU6VZoDPGGFOlWaAzxhhT\npVmgM8YYU6VZoDPGGFOliapWdhlKJSLrgGVxfpnGwB9xfo1EsveTvKrSewF7P8muKr+fP1S1b2kX\npESgSwQRma6q3Sq7HLFi7yd5VaX3AvZ+kp29H2u6/P/2zjzurunc499fJkISCUGDG6HGmslAJWoI\npYOhoqnhqvZeU9XsVg1X0UnLpUiNvS2l5tKLmlUkIZMQGQiRiJoacyQiQfLcP57neHdOzjnv+yZv\n3rPfbX0/n/M5++y9zt7Ps9dae631rLWfJ5FIJBIFJzV0iUQikSg0qaFr4Jp6C9DCJH3yS5F0gaRP\n3vnC65Pm6BKJRCJRaNKILpFIJBKFJjV0bRhJqrcMiUQikXe+MA2dpFUk/VLSgZLWi31ttqGQtCqw\ndmy3r7M4y4WkVSVdJGmXesvSEoQ+Z0kaLKlLveVZXkKfUyQNkrRyveVZXuJZMEzSbvWWpSWI/DlS\n0rb1lmV5CV0OlrRlS573C9HQSdoA+CvQCfgScJikla2NTlBK2hOYDfwZwMwW1VeiZSceNi8ALwJj\n6yzOciNpc+BBYA3ga8DebbkjImkQMAroDewL7C+pY32lWnYk9QQuxMvbE3UWZ7mR9CX8ObAZcJyk\n/SStUmexlglJ+wCjgV2BiySt3lLnLnRDJ2nt2JyLv0H/X8CdwGdmtqB+ki07ktoBbwE7AP+SdEDs\nb1MPU0ndYvNNvIF7Czg6KmqbK5eZsvYpMN7MTgEeAdZpix0RSWvF5pvAkWZ2EjAc2NjMPm1reSRp\ntdhcALwHTALOlDREUqf6SbZsZOrPR8A8MzsVuBzYEPhW3QRbBsp0+a6ZHQ08BrTYiLtDS50oT8QI\n7r+BbpJuxHtuIyT9FtgcWFvSbOBJM5shSXke3UnaCDgPOM/MXpQ0w8zmSboq9t9lZovyrgeApA2B\nc4H5km43s0cljQF+G/sPBVaTdK+ZvVc/SZuGpDWAC4Cekv6IuyaaIuksoC/QWdKLwDNmNruOojaJ\nCnVnuJm9FIcn4KOGjmb2ad2EbAYx4vkVsFjSLcBkvDNyOHAP3ih0kXR3GylvS9QfYCTwvKTtgYm4\ne6ydJK1lZm/VT9LGyeSNSbrJzB6N/T2B3YHekl4GJprZ4uW5VpvqlTUFSV2BHwPPAD/HC3I3M7sW\nmA88hw+NuwD7A+S5cQgzxDFAH+BMgGjkZGZPAtMlnVpKXh8pm0YU7J/gI7gb8R51J7wnup2Z3YQ3\nGpsAbcX8sjOwEDgW2ArYHrgFN11OA04EtgMG1kvAplJWd84Hvo3rUaIvMKGNjej2BN4HLgIOAPoD\n3YFPzewu4GK8bnWrdoK8UKH+/BRoj4+Etga64tMAneKTd0p5cyFwkKRvhGVqC3yqaTxeHrsu74Xa\nSmFtDvOBS8zscjN7FvgQGBzHXgdmm9l8vALPqo+IjZNZKPMxcL6Z7Qz0kFRyYFqaJzkR+KakE/A5\nodyR0eV94CIz+72ZjQJmAN8yswVm9hGAmT2NzzfkuqJmdHoD12s2cDv+wPwG8BLwmJm9gPeycz9a\nYMm6MwmYw5Lmo8+AiZIOAU7ImJxyRyZ/5gKTzGwacDewFvAq8L6kr5rZVHJe3mrUn1fwDtSD+MK0\n/czsdWAjILeLhqrkzR14A7e1mT1uZteY2XW41bH78l6zzTd0kraTtFPpd8yHvJGZs3oNH9KDj+a6\nS3oE73k/06rCNoEwHX0+yozv+XH4d8BJsf8TSR3wkemmeO/7tVYXuAYVdFkIzJTULkYEs/G5EiR1\nlDQg8uZtctgwSFqztJ2xAgivsJviDfckvAe6CDchPQKsB8xsXWkbJ6sPVK07kzNJ9se9UnwdeMjM\nPmwVQZtIZl6x3EqznqSVgCfxDuJk4HFgL0l/xzuTeSxva0HN+vMvYKaZvYibYTcPfT7FG8VcUWrg\nquTNGGAesE6kHSDpXrxufbDcFzezNvnBezCX4SO282Jfpwrp/oT34rrgPetVgUH1lr+CnGuEPqNw\nU+XGZcdLXmyuBg6O7ZXwXtC+9Za/Obpk0l2PP3i6A6sB3wUOqrf8FeTsEfrcAhwEdC87/hPge3gD\ntxVwTuxfB9iz3vI3V59Muj9FnVkjPvvgiwXqrkOZnF1Dn//D53h7lh2/Ehgc298Gfpn53471lr+5\n+mTSXR/PgB7xeyVgm3rLXyZjL7wRHhS/2wEdquTNN/Gpi3bAKcD+LSVHmxzRxVzC6bg9+mtAf0nt\nzOyTOP6fknaOnkJP4D+A+4ENzewjMxtZL9lrsB/eGO+Ny9wLvBck6WD8AQo+eXulpNHA+mY21czu\nrofANaipi6SvSOoR+48F7gPWMrPbzOz2egldgz1xU9DxwJeBg2M0TeTNh3hDfTo+2ukjqbOZvWFm\nD9dJ5lrU0ueosrpzJP7A7WNm95vZbfUSugZ98fJ2BD7fltVnH2Aq0E/SD3EzpUlayczmmtmY+ohc\nk1r6HFpWf44G7pW0uZktNJ+uyRMH4YOSHwOY2WIz+yxGpV9nybz5CrAo0lxsZn9rKSHaVEMn6ZuS\nDsJNDWea26qfwW9Wf0ntJZ2CT8xOxUcLfYD18ZHCuDqJXpHQ55D4+RYw3czm4b3nTaJBPw8YgJss\nOgDfx+cavmNussgFzdBlR3yOpCueNxsDB5rZ9NaXujpl+swFnjKzt/Flz7sD+0o6DtfnJnz08zqw\nOvBTM/u4DmJXpYn6/AjYkqXrzhAzm9D6UldHUjYe2b/wMjcXX6TREfihpJ/hjfr/xv618fcBLzE3\nA+aGJupzDtCPpevPEDN7vlUFroGkfvEt4Foz6w8sKJU/+asev8Y7hUvlzQqRKYaMuUfSicAu+OT/\nDHyO4Dn58u4zgavMbLqkHmb2fvynG26W+WfdBK9CRp838TmDGXjH4zx8Fd84vMCPNLPxmf91NbO5\nrS9xdZZFl1hB1i1PjXWJMn2exR80HfEHzCvACcDDwH3WNpakN1ufvNYduUeg23Azaj8zmyBpG9x6\ncD9e/vbA3yd7xMxmZv6bu9dvllWfPNafKrqsZGYL5V5bLsTNkR+p7PWHFZ03bWlE1x64zsyOx3s7\n+8dD/118QcAhAJlGrp2ZfZi3ipqhpM+PcdNXf3zZ8D/w1VM/wc1Lb4fJrx1A3hq5oNm6mNm/8lRJ\ny8jqMwefQ5yGm5TOxhc1fTnTKOS9HjVbn7zWHfPVuecAh9FgDnsW+AR3otAD93qymZnNLKs7uWrk\nYNn1yWP9KdPluNi3MOSdCDwF/CCSCxrqzorOm7xX0OxS1Fn48vqV8EUOC/FeKvgCjT7K+BW05XzB\ncEVRRZ8n8JWVh+AFfE9Jx+MT55+akzt9iqQLVNVnHG4qX2BmZ5rZAfjc8LzSvEnSZ8WT0QUzm2D+\nzmU3SQfG7geAzsAZeFnrJalLXstbkfSpostqCq9NeEcLfH3BOZKewl+BaLWylruGTv7S4IWSNoue\nQKmlfwdfpr0xPhcyl4Z3ydbAh/mftLrAjdAMfeYDL+MrlLaJz8Hm78XkgiLpAk3W51X8hdweMYF+\nBr5I4CYz+6wuglehyPpQNgIILsYXyxBzVH8CpuDziqfHHHFuKJI+TdTlaABr8KIzFB+kHGRmrepn\nNFdzdJJOw72WPIBP6o8ws+FxrBdwMD6SuwH4DrC6mV0clTpXvRxotj5DcJv77/KoT5F0gWUqaz3M\n7JI8zpHCF06ftYAPzN8l/QXuRWcWcLP5ApvcUSR9mqlLZ3we+CZgjtXJdVzdR3TZYS/+ouO9ZjYM\nd776WqQ5DX+/4gn8ZeI/4r3QyZAvU8ty6HM4vtoNIBe9jyLpAstd1qZAvuZIv6D6nAxcga/QA5/D\nGgrMz1ujUCR9lkOX7+G6vFOvRg7qPKILs8kWwFFmNl/SFrhLq1fxidlLcI8F04FxMdmJpK2Alyx/\nS7gLo0+RdIGkT4H1GQI8kCezHhRLnyLoUrcRnXx57I74/NpAAHO/c1fi7rn2wud4tgVeMF+S2jHS\nTc5hRS2MPkXSBZI+Bdfnjjw8SLMUSZ+i6NKqDZ2k3pLOlkePfcvM9sMjDOyvhnhe7+Pv+7yOLy4R\nbnLJTmrmgiLpUyRdIOlD0qdVKZI+RdKlRKs1dNEzuBT3x3YQ7voJ81Azc/EltOA3cCxu670amJbH\nG1ckfYqkCyR9SPq0KkXSp0i6LIGteKee3wL2BdYF7ol96+HvVJScE28JDMOd4G6ON8A7UsXZbD0/\nRdKnSLokfZI+SZ+kS7XPChvRSVpT0j34CrwhuF+zCZJ2NbPXgHuBr0rqZmZT4qaNxh0wdzazMWa2\n/OEZWogi6VMkXSDpQ9KnVSmSPkXSpRYr0nRpwDVm9l3c5+Fe+Lshm8njYE3C45H1kodOXxc4zcxO\ns1i1kzOKpE+RdIGkT9KndSmSPkXSpSorsqF7F498i5nNwH0gfozbfg81X42zCfCu+TsW+1k+Q7SU\nKJI+RdIFkj5Jn9alSPoUSZeqdFhRJzYzI1xyxQTnumZ2q9zD9aWS7sJX7cyX8udVvJwi6VMkXSDp\nk/RpXYqkT5F0qcUKa+hKSJ+/Uf8P+cuquwP/A8wzs1dX9PVbmiLpUyRdIOmTd5I++aVIulRihTd0\nZmaStgd+idt//2I5ChLYXIqkT5F0gaRP3kn65Jci6VKJVnEBJo+e+zXgcjPLXYSB5lIkfYqkCyR9\n8k7SJ78USZdyWquha7O23UoUSZ8i6QJJn7yT9MkvRdKlnFyF6UkkEolEoqWpe5ieRCKRSCRWJKmh\nSyQSiUShSQ1dIpFIJApNaugSiUQiUWhSQ5dIJBKJQlOXhk7SWZKmSpokaaKkAfWQoxqSjpA0rMqx\nMzPb3SX9qAWv21HShMzvqyXt3FLnzyuSdpV0b2xXvfcr8PoPSFp3Gf63r6SfZn73kvRQjfT9o7xP\nlPSspAPKji+V35L6SJrSiBx9JB3SXPkbQ9JJklZpQrqdJF0b2/0ljZD0gqRpkv4gaZVK+SppeLy7\nhaRZkibHM+FxSetn0s2L73aSLpM0JdKOl7RBJt0Zkg6N7cMj3VRJz0k6LfZfJ2m+pK6Z/10qyeRO\ni5G0KJNPE7N5XEH3jpIukDQ9rjdO0j4ZnXqWpd9M0mhJC0syJVY8rd7QSdoJj320vZltDQwGltvF\njKQV7uUlODOz3R1osYYOD1X/ZOb3AGBMC54/UYakzsDqZvZ6c/9rZneb2QWZXXsTDnKrMAXoa2bb\nRtqry8rtsuZ3H6DFGzrgJKDRhg7X5QF59OnbgdPNbFM8ZtkDQNdaf86wWzwThgNnVzg+FI+FtrWZ\nbQUcAGRDxOwFPBQNzUnAXma2BbA9MCeT7iVgP/DGE9gNj5Rd4mMz2zbzyeZxOT8HegFbmtmWeGDS\nWvq+B5wAXFQjTaKFqceIrhfwjpktBAiP2G8ASNpD0jPRW/ujpJVi/6xMb6uvpOGxfa6ka6IX/WdJ\n7SVdlOkZHh/pdohe4gRJD0rq1QQ514me/nRJv43zXAB0jl7eX4ALgC/H7wtjZDJC0l3Ri7wqeqHt\noydZ6omeXOWaewP3x7U2B14E1lCM8iRtEz3P3vF7hqRVJc2U013SYkm7xPGRkjaKa3aPNO9KOjyO\n3yBpcLkQ8hH3C5IekXRzpjec7YH3lDQrtvvEtZ6Oz1dj/67xnzvkvfu/SO5TT9LesW8U8J3GMqNG\n3tYqM7+K3vNTkraPvJ8h6ZjMqXfFH6xIOkc+SpgS5aok63BJv5P0ZBzrH/vLRyl7A/dLulXSNzKy\nXyfpQDObb2afxe6V8RAppTSbAy+a2aIor89KGg0cl0lT8T7j5XBQlMOTa+RHryifE0OPQbF/r7hP\nT0u6XVIXSSfgjcpjkh5rpAzvATwSsl5vZqPB3UqZ2R1mNrux/C1jNB4OppxewJtmtjjO/5qZvR86\ndAM6mdnbwBl4KJk3It0CM7s2c56b8UYTPP+fAD6jmchHu0cCx2eeZ7PN7LZq/zGzt8xsPFAzGrek\nflHenpWPErtK2iK2J0Yd2FjSb5SxKsmfiac2V5fCY60fybYLMBF/iF8BfC32r4yP7DaJ338GTort\nWUDP2O4LDI/tc4EJeABA8LDvfwU6xO/VgY74KGnN2DcU+GNsHwMcU0HGI4CZwGoh1yvAv8WxeZl0\nfYApmd+7AguADYH2wMN4MMMdgIcz6bpXuj4wDlgltk8BfhjbU4FuwI+B8cChwPrA6Dj+ALAFPlIe\nD5yFh9l4OY5fBXwTjxA8Hrg29k8HupTpvgMwGe/Jd8N7v6fFseH4iASgJzArtlcBVo7tjYGnMvdj\nDh6puBSwcWAmrzcGBNwG3Ju598Mq5EmlvG2szBwb25fgcbW6AmsCb2XOexmwe+mcmf03AN/O6F26\nZ7uU8jwra+T3xNg+AH/gA3QKGUtldEDk5zzggMz1svk9iYZ6cWHmerXu872Zc1VLdypwVkberpGP\nI4BVY//pwDkV6l21MtwTeCy27wT2q1LvjwDexut+6TOPhvKUvdbvgKMy/50X3+tFuom4w+HtMmm+\nA5wf2+8Bq1WR4zq8To4BegDX4m6vstdfVCbn0Crn2hp4psaz7vNzVjh2LlGvKhzrhD9/+sXvbrhf\n4svx0DmlNJ2B7YDHM/99DuhdTaYv6qfVR3Tm8Y12AI7CC/6tko4ANsUfzC9G0uvxh0pj3G1mH8f2\nYOAqi16zmb0X590SeFjSRNwksl4cv8rMrqpy3kfNbI6ZLcALz/pV0pUzzsxmmtkivOc4EC+0G0q6\nXNLeeMynJa4vaR3gPTObH+f5Ot6AgTfUO+P341fxPQgYGcdHxr5dgF/HNfvhjVr58SuBreRzUu9F\nfmQZBNxlPvr4ELi7CTp3BK6VNBk3XX2l7H68Zt4Ln4h3DjbD83q6ee28sQnXqJa3tcpMSfbJwFgz\nm2ve418gqXsc2xkYFdu7SRobeuyOdx5K3BzXHQF0y/y/xABgbGzfD+weo8t9gBGlMmpmY83Naf2A\nMyStHP/5Om7+93BieAAAChRJREFUWw1vRB6P/TdkrlHrPtOEdOOBH0g6F9jKzOYCO8bxJ6J+fJ/K\nZb1iGSbMhVXkKOdWy5gEgafKjj8m6S08r28q/7N5xOtN8RHbYuBRSXvE4c+tIU3kTuB7eL6NLDtW\nbrq8tRnnbQk2xUeu4wHM7MMo96OBMyWdDqxvZh+b2TPAWpLWkbQN8L6Z/bOV5c09dVmMYmaLzGy4\nmf0MH6UciPfsq/EZDbKuXHYsG+VWZMxBmX1TM4V2KzPbqwliLsxsL6LpkR7Kr2/m5pVt8JHBccAf\nKvxvH2J+J0wi3S1ML3hFHIQ/gP4vzjUQ74lnj/cH7sPnDnfNHB8RxweFDG/jvdryCl5NhxLV8uFk\nPArxNviIu1PmWLX72Fzfc9Xythalay8uk2Mx0EHShsCrZvZJNDhXAEPM53+uZUkdl8rXst/7EB2T\n6BwNxxuvocAt5YKZe4b/CNiyLL8r6Vmi1n1uNF000rvg81E3yE3YwkdqpfrxFTP7jwryVivDn+uN\nj1R3qCJTU9gNL+NTgfMrJTCzhWZ2v5n9F97p2z8O9cctIk2V4xZ8fu3h6IQtCy8BvZVZ2NJCVCwD\nZnYTsC8eGPVBSbvHoTvw+lyxrCXqsxhlU0kbZ3Zti5sGpwF9JG0U+/8dKPVqZ9FQcA+scfqHgGMU\nE/ySVgdeANaUL4IprZLaosY5GuNTSR1jey5LTzz3l7SBfJJ7KDBKPr/Yzsz+Cvw3PjleTrZHuhvw\nWObYCOAwYHpUyveAb+BzC+Ajia8Ci+MhOxE4mmjIzONJ9QQ2NrOZ+AjmNCo3dCOAAyR1jgr87cyx\nWTTkw5DM/tVomDv5d9wsVotpwAaSvhy/D24kPVTO21plpilkH9KlRu0dSV1YUj+IOR1JA4E5Zjan\n7PgewKOZ37cAP8A7F6UOzAYZ+dfHe+6zyOS3mX0AzInrgJupS1S7z+XlsGK6uOZb5vNV/4uXwzHA\nzqV7KF8huUn5eSuVYUnCzXcTI/0w4PvKrKKWdJg8oGeTiJHvScDhkcefI59nXSe228W1X4n6PC2s\nKOBWjd+WritppZhzzF7nn7iJ/4qmylZB1vn4fbxMUqe4Vi9Jhy3rOYNp+BqBfnHOrpJKHbOZZnYZ\nbq3YOtLfgo9Oh+CNXqKMeozougDXyxdrTMLNJufGA/oHwO1hclmMzy0BnIdHux2Jjwqq8Qfgn8Ak\nSc8Ch5iHmxgC/Cb2TcQbBSQdoyUXJjSFa+L8fzGzd3GTzxRJF8bx0fjigCnAy8Bd+MT68DANXYeb\nXj6/vqT2eCM0Lc6RfQBjZrNiszRCGwV8EL1szCfCX6Vhxd5I/AE1OSP3WHxetHR83TgPYfa4L871\nNHBr3Ke/smRjeBFwrKQn8YazxBX4A24MsAlLjrKXIvL6KODv8sUor1RKJ1++X+rZV8rbWmWmKexN\nwyjsA3wUNxn4Gw1m3xLvh95XAUuMeCStCSwIU2+Jh/DR0yPWEPJkIPBslIO7gB+Z2TuU5Xfo9Hv5\nYpSPM/ur3edJwGfyhQsn10i3KzBR0jN4h/HSMOUeAdwc9XEMbloGL+v3S3qMymV4B3yOyuIezsYf\nuBfJFzM9jzf02fvSKGb2Jm4qPq7s0FrAPfLXLSbhFoZhLF1f7gN+DzwiaSo+j7+URcbMrjazGRVE\nKC04K31qrbo8G7eQPBdy/S1+l5gk6bX4XCzpS5Jew+dkz4793QAk3SdpnSgvQ4HLo6w/jHfEhgJT\nIg82w+ekMbOpeH1/Pe5doowUvaAFkbQrPsH8rWb+byBwmJkdE7+fBgaYWc2VWa2BfD5nnpkVajm0\nfP7sCTPr24S0w/F8LZ9TKh0/DFjPai9Dr3X+3OR3c5B0NvCSmdXVXCbpYeDw9JBPVKO13j1L1MDM\nRtGwIAIzq2TaTLQgMQputJFr4rmaspim1v/bZH6b2S/qLQOAme1ZbxkS+SaN6BKJRKIRJN0FbFC2\n+3Qzq+UgIJETUkOXSCQSiUKTnDonEolEotCkhi6RSCQShaZe0Qsa9cge6c5XBV+My3jNz/005hm5\nP8Hyd7gSiUQisYzkdtWlpPZmdk695ViRSOpgDU5+63aORCKRKDL1NF12kHS93Av3HeGRYZbcg/wo\n4KDs6Ea1Pcv/Ru7V+0U1eGTvLOmWOP+tuANU4lglb+39Jd0Zx/eT9LGkTpJWljSzXHg1EhEg/ven\nSPeMpN3i+BFxzXvwkCKSNEz+Av3f8ZdiS9eYldFtnBq8V1wXL58+hr8Iv6rcc//4uFYpBEklb+er\nSvq7/OXiKZKGVtBtbXkEhmfjU3rB/pT4zxRJJ8W+PmqIOzZFHqFgsKQn5JEf+i93SUkkEonloJ4N\n3abANebxpz6kIa7bAjMbWOEl1GFm1s885lNn3FN/iQ5m1h93HfSz2HcsMD/O/0vCdZXcldHZwOB4\nf+kp3EvB07gncHBvDlNwx7tZZ71ZnsAdAm+BO7wdFPt3xL1LHAcQfhMPxr3BlNxM7QR838x2xz3d\nbwpshYf8KIVeKfFh6DYM9+peYpPQ4VTcldE/zKwf7k7qQkmr4tERLg0Hun2B13BvIG+Y2TZxL7Me\nOUpchntE3wZ3EzVV0g64x44BoeORkkr3ayPgUtwl0WZ4bLSBuJuxM0kkEok6Us+G7lUzK/lqvBF/\nMIK7n6rEbqruWf7O+J6Ae8cHd790I4CZTcJdBkEVb+1h/ntJHhesP3AxS0cJyNJYRICBhOf5cO31\nCt44gTuSfS8j583h6PoN4B9l17k5871TZv/tGd9+ewE/DX2G4+6CelPB2znu4mpwjBQHVfDZCH5/\nrwzZF0WagXhUg49CvztpaNxfNrPJ4VtxKh75weJafSqcP5FIJFqNejZ01bzBL+UnUY17li95pi+P\nMlDpJcFa3tpH4n7zPsUDSQ5kySgBWRqLCFDLs365jrVeZrQq2+VRGw7M6NTbzJ6v5O3cPKRNKebc\nryU1dR60lj7lkQGyUQNyOw+cSCS+GNSzoeutiCiAm/ZG1UjbmGf5SowgPL9L2pIGT9+1vLWPwM2f\no8PZ7Rq4KW5q+cmbEBEge/1N8BHWC1Xk/J48gnMv3PSYZWjme3QVXR8Ejs/MW24X30t5O5d7f58f\nbqsuonIkhUdx028psne3kHP/uF+r4ibXamF+EolEIjfUs6F7HvewPgmPFn1ltYRN8CxfiSuBLnH+\nnxCxqhrx1j4WWJuGEdwkYFLJO7uWjnZQNSIAPgJtH6bWW4Ejwr9iOXfhkb4nh8zlYWZWkjQWOBGP\nM1aJn+PBNifJX9v4eeyv5O18K2Bc7DsL+EXodr6kfeN/J+Km4sm4OXiLiGpwHX4fxwJ/MA/6mEgk\nErkmuQDLMZJmAX0jlEsikUgkloHkGSWRSCQShSaN6BKJRCJRaNKILpFIJBKFJjV0iUQikSg0qaFL\nJBKJRKFJDV0ikUgkCk1q6BKJRCJRaFJDl0gkEolC8/+yXS6Bfq3hgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23b1e043390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=[7,5])\n",
    "ax1 = plt.subplot(111)\n",
    "line = wticl1_last.tail(68).plot(color='red',linewidth=3)\n",
    "ax1.set_ylabel('USD per barrel')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_title('WTI Crude Oil Price', fontsize=18)\n",
    "ax1.spines[\"top\"].set_visible(False)  \n",
    "ax1.spines[\"right\"].set_visible(False)  \n",
    "ax1.get_xaxis().tick_bottom()\n",
    "ax1.get_yaxis().tick_left()\n",
    "ax1.tick_params(axis='x', which='major', labelsize=8)\n",
    "fig.text(0.15, 0.85,'Last: $' + str(wticl1.Last[-1])\\\n",
    "         + ' (as of: ' \\\n",
    "         + str(wticl1.index[-1].strftime('%Y-%m-%d'))\\\n",
    "         + ')');\n",
    "fig.text(0.15, 0.80,'Change: $' + str(wticl1.Change[-1])\\\n",
    "         + '; ' \\\n",
    "         + str((np.round((wticl1.PctCh[-1] * 100), \\\n",
    "         decimals=2))) + '%')\n",
    "fig.text(0.1, 0.06, 'Source: ' + url)\n",
    "fig.text(0.1, 0.02, 'briandew.wordpress.com')\n",
    "plt.savefig('charts/oil.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeldata = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Change</th>\n",
       "      <th>Settle</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous Day Open Interest</th>\n",
       "      <th>PctCh</th>\n",
       "      <th>...</th>\n",
       "      <th>NATR_100</th>\n",
       "      <th>NATR_200</th>\n",
       "      <th>TR</th>\n",
       "      <th>chaikinAD</th>\n",
       "      <th>chaikinAD_OSC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>avgprice</th>\n",
       "      <th>medprice</th>\n",
       "      <th>typprice</th>\n",
       "      <th>wclprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1983-03-30</td>\n",
       "      <td>29.01</td>\n",
       "      <td>29.56</td>\n",
       "      <td>29.01</td>\n",
       "      <td>29.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.40</td>\n",
       "      <td>949.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>396.854545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>949.0</td>\n",
       "      <td>29.2450</td>\n",
       "      <td>29.285</td>\n",
       "      <td>29.323333</td>\n",
       "      <td>29.3425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983-03-31</td>\n",
       "      <td>29.40</td>\n",
       "      <td>29.60</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.29</td>\n",
       "      <td>521.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>-0.003741</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-5.059740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>428.0</td>\n",
       "      <td>29.3850</td>\n",
       "      <td>29.425</td>\n",
       "      <td>29.380000</td>\n",
       "      <td>29.3575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983-04-04</td>\n",
       "      <td>29.30</td>\n",
       "      <td>29.70</td>\n",
       "      <td>29.29</td>\n",
       "      <td>29.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.44</td>\n",
       "      <td>156.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-46.913399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>584.0</td>\n",
       "      <td>29.4325</td>\n",
       "      <td>29.495</td>\n",
       "      <td>29.476667</td>\n",
       "      <td>29.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1983-04-05</td>\n",
       "      <td>29.50</td>\n",
       "      <td>29.80</td>\n",
       "      <td>29.50</td>\n",
       "      <td>29.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.71</td>\n",
       "      <td>175.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>23.086601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>759.0</td>\n",
       "      <td>29.6275</td>\n",
       "      <td>29.650</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1983-04-06</td>\n",
       "      <td>29.90</td>\n",
       "      <td>29.92</td>\n",
       "      <td>29.65</td>\n",
       "      <td>29.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.90</td>\n",
       "      <td>392.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.006395</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27</td>\n",
       "      <td>357.012527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>29.8425</td>\n",
       "      <td>29.785</td>\n",
       "      <td>29.823333</td>\n",
       "      <td>29.8425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Open   High    Low  Close  Change  Settle  Volume  \\\n",
       "0  1983-03-30  29.01  29.56  29.01  29.40     NaN   29.40   949.0   \n",
       "1  1983-03-31  29.40  29.60  29.25  29.29     NaN   29.29   521.0   \n",
       "2  1983-04-04  29.30  29.70  29.29  29.44     NaN   29.44   156.0   \n",
       "3  1983-04-05  29.50  29.80  29.50  29.71     NaN   29.71   175.0   \n",
       "4  1983-04-06  29.90  29.92  29.65  29.90     NaN   29.90   392.0   \n",
       "\n",
       "   Previous Day Open Interest     PctCh    ...     NATR_100  NATR_200    TR  \\\n",
       "0                       470.0       NaN    ...          NaN       NaN   NaN   \n",
       "1                       523.0 -0.003741    ...          NaN       NaN  0.35   \n",
       "2                       583.0  0.005121    ...          NaN       NaN  0.41   \n",
       "3                       623.0  0.009171    ...          NaN       NaN  0.36   \n",
       "4                       640.0  0.006395    ...          NaN       NaN  0.27   \n",
       "\n",
       "    chaikinAD  chaikinAD_OSC     OBV  avgprice  medprice   typprice  wclprice  \n",
       "0  396.854545            NaN   949.0   29.2450    29.285  29.323333   29.3425  \n",
       "1   -5.059740            NaN   428.0   29.3850    29.425  29.380000   29.3575  \n",
       "2  -46.913399            NaN   584.0   29.4325    29.495  29.476667   29.4675  \n",
       "3   23.086601            NaN   759.0   29.6275    29.650  29.670000   29.6800  \n",
       "4  357.012527            NaN  1151.0   29.8425    29.785  29.823333   29.8425  \n",
       "\n",
       "[5 rows x 181 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8907, 181)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeldata = modeldata.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Previous Day Open Interest',\n",
       "       'PctCh', 'bbands_high', 'bbands_mid', 'bbands_low',\n",
       "       ...\n",
       "       'NATR_100', 'NATR_200', 'TR', 'chaikinAD', 'chaikinAD_OSC', 'OBV',\n",
       "       'avgprice', 'medprice', 'typprice', 'wclprice'],\n",
       "      dtype='object', length=178)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct change and change are same columns\n",
    "#settle and close are same columns\n",
    "#modeldata.pop('Change')\n",
    "#modeldata.pop('Settle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous Day Open Interest</th>\n",
       "      <th>PctCh</th>\n",
       "      <th>bbands_high</th>\n",
       "      <th>bbands_mid</th>\n",
       "      <th>bbands_low</th>\n",
       "      <th>...</th>\n",
       "      <th>NATR_100</th>\n",
       "      <th>NATR_200</th>\n",
       "      <th>TR</th>\n",
       "      <th>chaikinAD</th>\n",
       "      <th>chaikinAD_OSC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>avgprice</th>\n",
       "      <th>medprice</th>\n",
       "      <th>typprice</th>\n",
       "      <th>wclprice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1983-03-30</th>\n",
       "      <td>29.01</td>\n",
       "      <td>29.56</td>\n",
       "      <td>29.01</td>\n",
       "      <td>29.40</td>\n",
       "      <td>949.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>396.854545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>949.0</td>\n",
       "      <td>29.2450</td>\n",
       "      <td>29.285</td>\n",
       "      <td>29.323333</td>\n",
       "      <td>29.3425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-03-31</th>\n",
       "      <td>29.40</td>\n",
       "      <td>29.60</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.29</td>\n",
       "      <td>521.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>-0.374150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-5.059740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>428.0</td>\n",
       "      <td>29.3850</td>\n",
       "      <td>29.425</td>\n",
       "      <td>29.380000</td>\n",
       "      <td>29.3575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-04-04</th>\n",
       "      <td>29.30</td>\n",
       "      <td>29.70</td>\n",
       "      <td>29.29</td>\n",
       "      <td>29.44</td>\n",
       "      <td>156.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0.512120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-46.913399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>584.0</td>\n",
       "      <td>29.4325</td>\n",
       "      <td>29.495</td>\n",
       "      <td>29.476667</td>\n",
       "      <td>29.4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-04-05</th>\n",
       "      <td>29.50</td>\n",
       "      <td>29.80</td>\n",
       "      <td>29.50</td>\n",
       "      <td>29.71</td>\n",
       "      <td>175.0</td>\n",
       "      <td>623.0</td>\n",
       "      <td>0.917120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.36</td>\n",
       "      <td>23.086601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>759.0</td>\n",
       "      <td>29.6275</td>\n",
       "      <td>29.650</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.6800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-04-06</th>\n",
       "      <td>29.90</td>\n",
       "      <td>29.92</td>\n",
       "      <td>29.65</td>\n",
       "      <td>29.90</td>\n",
       "      <td>392.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.639515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27</td>\n",
       "      <td>357.012527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>29.8425</td>\n",
       "      <td>29.785</td>\n",
       "      <td>29.823333</td>\n",
       "      <td>29.8425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close  Volume  Previous Day Open Interest  \\\n",
       "Date                                                                         \n",
       "1983-03-30  29.01  29.56  29.01  29.40   949.0                       470.0   \n",
       "1983-03-31  29.40  29.60  29.25  29.29   521.0                       523.0   \n",
       "1983-04-04  29.30  29.70  29.29  29.44   156.0                       583.0   \n",
       "1983-04-05  29.50  29.80  29.50  29.71   175.0                       623.0   \n",
       "1983-04-06  29.90  29.92  29.65  29.90   392.0                       640.0   \n",
       "\n",
       "               PctCh  bbands_high  bbands_mid  bbands_low    ...     NATR_100  \\\n",
       "Date                                                         ...                \n",
       "1983-03-30       NaN          NaN         NaN         NaN    ...          NaN   \n",
       "1983-03-31 -0.374150          NaN         NaN         NaN    ...          NaN   \n",
       "1983-04-04  0.512120          NaN         NaN         NaN    ...          NaN   \n",
       "1983-04-05  0.917120          NaN         NaN         NaN    ...          NaN   \n",
       "1983-04-06  0.639515          NaN         NaN         NaN    ...          NaN   \n",
       "\n",
       "            NATR_200    TR   chaikinAD  chaikinAD_OSC     OBV  avgprice  \\\n",
       "Date                                                                      \n",
       "1983-03-30       NaN   NaN  396.854545            NaN   949.0   29.2450   \n",
       "1983-03-31       NaN  0.35   -5.059740            NaN   428.0   29.3850   \n",
       "1983-04-04       NaN  0.41  -46.913399            NaN   584.0   29.4325   \n",
       "1983-04-05       NaN  0.36   23.086601            NaN   759.0   29.6275   \n",
       "1983-04-06       NaN  0.27  357.012527            NaN  1151.0   29.8425   \n",
       "\n",
       "            medprice   typprice  wclprice  \n",
       "Date                                       \n",
       "1983-03-30    29.285  29.323333   29.3425  \n",
       "1983-03-31    29.425  29.380000   29.3575  \n",
       "1983-04-04    29.495  29.476667   29.4675  \n",
       "1983-04-05    29.650  29.670000   29.6800  \n",
       "1983-04-06    29.785  29.823333   29.8425  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8907, 178)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to predict next day price movement\n",
    "modeldata.PctCh = modeldata.PctCh.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Previous Day Open Interest</th>\n",
       "      <th>PctCh</th>\n",
       "      <th>bbands_high</th>\n",
       "      <th>bbands_mid</th>\n",
       "      <th>bbands_low</th>\n",
       "      <th>...</th>\n",
       "      <th>NATR_100</th>\n",
       "      <th>NATR_200</th>\n",
       "      <th>TR</th>\n",
       "      <th>chaikinAD</th>\n",
       "      <th>chaikinAD_OSC</th>\n",
       "      <th>OBV</th>\n",
       "      <th>avgprice</th>\n",
       "      <th>medprice</th>\n",
       "      <th>typprice</th>\n",
       "      <th>wclprice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>70.06</td>\n",
       "      <td>70.36</td>\n",
       "      <td>69.64</td>\n",
       "      <td>69.88</td>\n",
       "      <td>416101.0</td>\n",
       "      <td>413062.0</td>\n",
       "      <td>-0.729823</td>\n",
       "      <td>70.814882</td>\n",
       "      <td>67.720000</td>\n",
       "      <td>64.625118</td>\n",
       "      <td>...</td>\n",
       "      <td>2.231932</td>\n",
       "      <td>2.155690</td>\n",
       "      <td>0.72</td>\n",
       "      <td>5.940741e+07</td>\n",
       "      <td>322965.120560</td>\n",
       "      <td>22330010.0</td>\n",
       "      <td>69.9850</td>\n",
       "      <td>70.000</td>\n",
       "      <td>69.96</td>\n",
       "      <td>69.9400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-04</th>\n",
       "      <td>69.89</td>\n",
       "      <td>71.40</td>\n",
       "      <td>69.08</td>\n",
       "      <td>69.37</td>\n",
       "      <td>782807.0</td>\n",
       "      <td>412237.0</td>\n",
       "      <td>-1.052328</td>\n",
       "      <td>71.046034</td>\n",
       "      <td>67.850667</td>\n",
       "      <td>64.655299</td>\n",
       "      <td>...</td>\n",
       "      <td>2.259302</td>\n",
       "      <td>2.177403</td>\n",
       "      <td>2.32</td>\n",
       "      <td>5.882031e+07</td>\n",
       "      <td>75931.201659</td>\n",
       "      <td>21547203.0</td>\n",
       "      <td>69.9350</td>\n",
       "      <td>70.240</td>\n",
       "      <td>69.95</td>\n",
       "      <td>69.8050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-05</th>\n",
       "      <td>69.43</td>\n",
       "      <td>69.59</td>\n",
       "      <td>68.56</td>\n",
       "      <td>68.64</td>\n",
       "      <td>537722.0</td>\n",
       "      <td>408792.0</td>\n",
       "      <td>-1.092657</td>\n",
       "      <td>71.132261</td>\n",
       "      <td>67.984667</td>\n",
       "      <td>64.837072</td>\n",
       "      <td>...</td>\n",
       "      <td>2.275502</td>\n",
       "      <td>2.197060</td>\n",
       "      <td>1.03</td>\n",
       "      <td>5.836612e+07</td>\n",
       "      <td>-176546.719066</td>\n",
       "      <td>21009481.0</td>\n",
       "      <td>69.0550</td>\n",
       "      <td>69.075</td>\n",
       "      <td>68.93</td>\n",
       "      <td>68.8575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-06</th>\n",
       "      <td>68.64</td>\n",
       "      <td>69.02</td>\n",
       "      <td>67.00</td>\n",
       "      <td>67.89</td>\n",
       "      <td>624741.0</td>\n",
       "      <td>398883.0</td>\n",
       "      <td>-0.044189</td>\n",
       "      <td>70.869783</td>\n",
       "      <td>68.184000</td>\n",
       "      <td>65.498217</td>\n",
       "      <td>...</td>\n",
       "      <td>2.307388</td>\n",
       "      <td>2.225102</td>\n",
       "      <td>2.02</td>\n",
       "      <td>5.829189e+07</td>\n",
       "      <td>-287401.011004</td>\n",
       "      <td>20384740.0</td>\n",
       "      <td>68.1375</td>\n",
       "      <td>68.010</td>\n",
       "      <td>67.97</td>\n",
       "      <td>67.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>67.88</td>\n",
       "      <td>68.08</td>\n",
       "      <td>66.86</td>\n",
       "      <td>67.86</td>\n",
       "      <td>517770.0</td>\n",
       "      <td>378062.0</td>\n",
       "      <td>-0.486295</td>\n",
       "      <td>70.615631</td>\n",
       "      <td>68.344000</td>\n",
       "      <td>66.072369</td>\n",
       "      <td>...</td>\n",
       "      <td>2.303302</td>\n",
       "      <td>2.223944</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5.862292e+07</td>\n",
       "      <td>-201294.358942</td>\n",
       "      <td>19866970.0</td>\n",
       "      <td>67.6700</td>\n",
       "      <td>67.470</td>\n",
       "      <td>67.60</td>\n",
       "      <td>67.6650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close    Volume  Previous Day Open Interest  \\\n",
       "Date                                                                           \n",
       "2018-08-31  70.06  70.36  69.64  69.88  416101.0                    413062.0   \n",
       "2018-09-04  69.89  71.40  69.08  69.37  782807.0                    412237.0   \n",
       "2018-09-05  69.43  69.59  68.56  68.64  537722.0                    408792.0   \n",
       "2018-09-06  68.64  69.02  67.00  67.89  624741.0                    398883.0   \n",
       "2018-09-07  67.88  68.08  66.86  67.86  517770.0                    378062.0   \n",
       "\n",
       "               PctCh  bbands_high  bbands_mid  bbands_low    ...     NATR_100  \\\n",
       "Date                                                         ...                \n",
       "2018-08-31 -0.729823    70.814882   67.720000   64.625118    ...     2.231932   \n",
       "2018-09-04 -1.052328    71.046034   67.850667   64.655299    ...     2.259302   \n",
       "2018-09-05 -1.092657    71.132261   67.984667   64.837072    ...     2.275502   \n",
       "2018-09-06 -0.044189    70.869783   68.184000   65.498217    ...     2.307388   \n",
       "2018-09-07 -0.486295    70.615631   68.344000   66.072369    ...     2.303302   \n",
       "\n",
       "            NATR_200    TR     chaikinAD  chaikinAD_OSC         OBV  avgprice  \\\n",
       "Date                                                                            \n",
       "2018-08-31  2.155690  0.72  5.940741e+07  322965.120560  22330010.0   69.9850   \n",
       "2018-09-04  2.177403  2.32  5.882031e+07   75931.201659  21547203.0   69.9350   \n",
       "2018-09-05  2.197060  1.03  5.836612e+07 -176546.719066  21009481.0   69.0550   \n",
       "2018-09-06  2.225102  2.02  5.829189e+07 -287401.011004  20384740.0   68.1375   \n",
       "2018-09-07  2.223944  1.22  5.862292e+07 -201294.358942  19866970.0   67.6700   \n",
       "\n",
       "            medprice  typprice  wclprice  \n",
       "Date                                      \n",
       "2018-08-31    70.000     69.96   69.9400  \n",
       "2018-09-04    70.240     69.95   69.8050  \n",
       "2018-09-05    69.075     68.93   68.8575  \n",
       "2018-09-06    68.010     67.97   67.9500  \n",
       "2018-09-07    67.470     67.60   67.6650  \n",
       "\n",
       "[5 rows x 178 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop last row with percent change Nan due to shift\n",
    "modeldata.drop(modeldata.tail(1).index,inplace=True)\n",
    "modeldata.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeldata_prediction = modeldata['2018-08-11':] # test data for all Models (except LSTM)\n",
    "modeldata_prediction_LSTM = modeldata['2018-08-11':] # prediction data for only LSTM\n",
    "modeldata = modeldata['1983-03-30':'2018-08-10':] # train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeldata_prediction_percentages = modeldata_prediction['PctCh'].to_frame()\n",
    "modeldata_prediction_percentages['Target'] = pd.cut(modeldata_prediction_percentages['PctCh'],bins=[-25,-2,-0.5,0.5,2,25], labels=[-2,-1,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2018-08-13   -1.157098\n",
       "2018-08-14   -2.596428\n",
       "2018-08-15    0.862866\n",
       "2018-08-16    0.702719\n",
       "2018-08-17    1.031553\n",
       "2018-08-20    1.081081\n",
       "2018-08-21    1.173500\n",
       "2018-08-22   -0.381735\n",
       "2018-08-23    0.987472\n",
       "2018-08-24    0.583771\n",
       "2018-08-27   -0.565873\n",
       "2018-08-28    1.707281\n",
       "2018-08-29    0.502152\n",
       "2018-08-30   -0.242684\n",
       "2018-08-31   -0.729823\n",
       "2018-09-04   -1.052328\n",
       "2018-09-05   -1.092657\n",
       "2018-09-06   -0.044189\n",
       "2018-09-07   -0.486295\n",
       "Name: PctCh, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata_prediction.pop('PctCh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = modeldata\n",
    "final_prediction_LSTM = modeldata_prediction_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final.dropna(inplace=True)\n",
    "final_prediction_LSTM.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final['Target'] = pd.cut(final['percentage_change'],bins=[-25,-2.5,-0.01,0.01,2.5,25], labels=[-2,-1,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5 bins classification\n",
    "#final['Target'] = pd.cut(final['PctCh'],bins=[-50,-4.6577,-1.1897,0.9769,4.179,50], labels=[-2,-1,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final['Target'] = pd.cut(final['PctCh'],bins=[-25,-2,-0.5,0.5,2,25], labels=[-2,-1,0,1,2])\n",
    "final_prediction_LSTM['Target'] = pd.cut(final_prediction_LSTM['PctCh'],bins=[-25,-2,-0.5,0.5,2,25], labels=[-2,-1,0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2018-08-13   -1.157098\n",
       "2018-08-14   -2.596428\n",
       "2018-08-15    0.862866\n",
       "2018-08-16    0.702719\n",
       "2018-08-17    1.031553\n",
       "2018-08-20    1.081081\n",
       "2018-08-21    1.173500\n",
       "2018-08-22   -0.381735\n",
       "2018-08-23    0.987472\n",
       "2018-08-24    0.583771\n",
       "2018-08-27   -0.565873\n",
       "2018-08-28    1.707281\n",
       "2018-08-29    0.502152\n",
       "2018-08-30   -0.242684\n",
       "2018-08-31   -0.729823\n",
       "2018-09-04   -1.052328\n",
       "2018-09-05   -1.092657\n",
       "2018-09-06   -0.044189\n",
       "2018-09-07   -0.486295\n",
       "Name: PctCh, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.pop('PctCh')\n",
    "final_prediction_LSTM.pop('PctCh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final.dropna(inplace=True)\n",
    "final_prediction_LSTM.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used for LSTM\n",
    "final_before_model = final.copy(deep=True)\n",
    "final_prediction = final_prediction_LSTM.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = final.pop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "1985-08-16    0\n",
       "1985-08-19    0\n",
       "1985-08-20   -1\n",
       "1985-08-21    0\n",
       "1985-08-22    0\n",
       "Name: Target, dtype: category\n",
       "Categories (5, int64): [-2 < -1 < 0 < 1 < 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6629, 177) (1658, 177) (6629,) (1658,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.24\n",
      "[[  39  279   83  743   37]\n",
      " [  34  401  189 1281   27]\n",
      " [  24  415  162 1248   25]\n",
      " [  28  443  189 1332   35]\n",
      " [  33  279   82  825   54]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.25      0.03      0.06      1181\n",
      "         -1       0.22      0.21      0.21      1932\n",
      "          0       0.23      0.09      0.13      1874\n",
      "          1       0.25      0.66      0.36      2027\n",
      "          2       0.30      0.04      0.07      1273\n",
      "\n",
      "avg / total       0.25      0.24      0.19      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, Y)\n",
    "#LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg.score(X,Y)))\n",
    "\n",
    "pred = logreg.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658, 177)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1658,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on test set: 0.23\n",
      "[[  8  65  15 144  11]\n",
      " [  8  88  41 261   5]\n",
      " [  5  86  23 239   7]\n",
      " [  4  95  34 255   7]\n",
      " [  8  55  12 174   8]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.24      0.03      0.06       243\n",
      "         -1       0.23      0.22      0.22       403\n",
      "          0       0.18      0.06      0.09       360\n",
      "          1       0.24      0.65      0.35       395\n",
      "          2       0.21      0.03      0.05       257\n",
      "\n",
      "avg / total       0.22      0.23      0.17      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = logreg.predict(X_test)\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg.score(X_test,Y_test)))\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeldata_prediction = modeldata_prediction.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 5 0 0]\n",
      " [0 4 0 0]\n",
      " [0 7 2 0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.29      1.00      0.45         5\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.08      0.26      0.12        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "log_prediction = logreg.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],log_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],log_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TUNING LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\optimize\\linesearch.py:461: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\vishali\\AppData\\Roaming\\Python\\Python36\\site-packages\\scipy\\optimize\\linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on training set: 0.26\n",
      "[[346 123 228 151 333]\n",
      " [321 290 598 285 438]\n",
      " [274 217 756 267 360]\n",
      " [334 247 644 331 471]\n",
      " [288 153 230 141 461]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.22      0.29      0.25      1181\n",
      "         -1       0.28      0.15      0.20      1932\n",
      "          0       0.31      0.40      0.35      1874\n",
      "          1       0.28      0.16      0.21      2027\n",
      "          2       0.22      0.36      0.28      1273\n",
      "\n",
      "avg / total       0.27      0.26      0.25      8287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg_new = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=12, fit_intercept=True, intercept_scaling=1, class_weight='balanced', \n",
    "                            random_state=None, solver='newton-cg', max_iter=30, multi_class='multinomial', verbose=0, warm_start=False, n_jobs=1)\n",
    "logreg_new.fit(X, Y)\n",
    "#LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(logreg_new.score(X,Y)))\n",
    "\n",
    "pred = logreg_new.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic regression classifier on test set: 0.27\n",
      "[[ 72  28  48  35  60]\n",
      " [ 70  62 129  54  88]\n",
      " [ 49  36 147  55  73]\n",
      " [ 59  47 121  66 102]\n",
      " [ 66  27  38  31  95]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.23      0.30      0.26       243\n",
      "         -1       0.31      0.15      0.21       403\n",
      "          0       0.30      0.41      0.35       360\n",
      "          1       0.27      0.17      0.21       395\n",
      "          2       0.23      0.37      0.28       257\n",
      "\n",
      "avg / total       0.28      0.27      0.26      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = logreg_new.predict(X_test)\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(logreg_new.score(X_test,Y_test)))\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [0 0 5 0]\n",
      " [0 0 4 0]\n",
      " [0 1 7 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.24      1.00      0.38         4\n",
      "          1       1.00      0.11      0.20         9\n",
      "\n",
      "avg / total       0.52      0.26      0.17        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "logreg_new_prediction = logreg_new.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],logreg_new_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],logreg_new_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.25\n",
      "[[  60  751  145  200   25]\n",
      " [  54 1266  350  252   10]\n",
      " [  38 1199  397  233    7]\n",
      " [  42 1314  378  285    8]\n",
      " [  56  867  133  190   27]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.24      0.05      0.08      1181\n",
      "         -1       0.23      0.66      0.35      1932\n",
      "          0       0.28      0.21      0.24      1874\n",
      "          1       0.25      0.14      0.18      2027\n",
      "          2       0.35      0.02      0.04      1273\n",
      "\n",
      "avg / total       0.27      0.25      0.20      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X,Y)\n",
    "print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "     .format(gnb.score(X,Y)))\n",
    "\n",
    "pred = gnb.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on test set: 0.24\n",
      "[[  7 148  37  44   7]\n",
      " [ 11 257  80  54   1]\n",
      " [  7 231  70  52   0]\n",
      " [  9 249  78  57   2]\n",
      " [ 13 168  27  43   6]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.15      0.03      0.05       243\n",
      "         -1       0.24      0.64      0.35       403\n",
      "          0       0.24      0.19      0.21       360\n",
      "          1       0.23      0.14      0.18       395\n",
      "          2       0.38      0.02      0.04       257\n",
      "\n",
      "avg / total       0.25      0.24      0.19      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = gnb.predict(X_test)\n",
    "print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "     .format(gnb.score(X_test,Y_test)))\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0]\n",
      " [0 0 5 0]\n",
      " [0 0 4 0]\n",
      " [0 0 9 0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.21      1.00      0.35         4\n",
      "          1       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.04      0.21      0.07        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "gnb_prediction = gnb.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],gnb_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],gnb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.predict(modeldata_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR DISCRIMINANT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA classifier on training set: 0.33\n",
      "[[246 213 168 377 177]\n",
      " [103 560 489 613 167]\n",
      " [ 79 413 683 589 110]\n",
      " [108 424 459 890 146]\n",
      " [124 273 164 385 327]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.37      0.21      0.27      1181\n",
      "         -1       0.30      0.29      0.29      1932\n",
      "          0       0.35      0.36      0.36      1874\n",
      "          1       0.31      0.44      0.36      2027\n",
      "          2       0.35      0.26      0.30      1273\n",
      "\n",
      "avg / total       0.33      0.33      0.32      8287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X,Y)\n",
    "print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "     .format(lda.score(X,Y)))\n",
    "\n",
    "pred = lda.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  44  42  68  30]\n",
      " [ 21 122 100 121  39]\n",
      " [ 13  72 127 121  27]\n",
      " [ 20  76  96 168  35]\n",
      " [ 32  48  25  74  78]]\n",
      "Accuracy of LDA classifier on test set: 0.33\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.41      0.24      0.30       243\n",
      "         -1       0.34      0.30      0.32       403\n",
      "          0       0.33      0.35      0.34       360\n",
      "          1       0.30      0.43      0.35       395\n",
      "          2       0.37      0.30      0.33       257\n",
      "\n",
      "avg / total       0.34      0.33      0.33      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = lda.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print(prediction)\n",
    "\n",
    "print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "     .format(lda.score(X_test,Y_test)))\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[0 0 1 0]\n",
      " [0 0 5 0]\n",
      " [0 0 4 0]\n",
      " [0 0 9 0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.21      1.00      0.35         4\n",
      "          1       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.04      0.21      0.07        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lda_prediction = lda.predict(modeldata_prediction)\n",
    "print(lda_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],lda_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],lda_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 1.00\n",
      "[[1181    0    0    0    0]\n",
      " [   0 1932    0    0    0]\n",
      " [   0    0 1874    0    0]\n",
      " [   0    0    0 2027    0]\n",
      " [   0    0    0    0 1273]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       1.00      1.00      1.00      1181\n",
      "         -1       1.00      1.00      1.00      1932\n",
      "          0       1.00      1.00      1.00      1874\n",
      "          1       1.00      1.00      1.00      2027\n",
      "          2       1.00      1.00      1.00      1273\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X,Y)\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(svm.score(X,Y)))\n",
    "\n",
    "pred = svm.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on test set: 1.00\n",
      "[[243   0   0   0   0]\n",
      " [  0 403   0   0   0]\n",
      " [  0   0 360   0   0]\n",
      " [  0   0   0 395   0]\n",
      " [  0   0   0   0 257]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       1.00      1.00      1.00       243\n",
      "         -1       1.00      1.00      1.00       403\n",
      "          0       1.00      1.00      1.00       360\n",
      "          1       1.00      1.00      1.00       395\n",
      "          2       1.00      1.00      1.00       257\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = svm.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm.score(X_test,Y_test)))\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict(modeldata_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0 0 0 1]\n",
      " [0 0 0 5]\n",
      " [0 0 0 4]\n",
      " [0 0 0 9]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       0.47      1.00      0.64         9\n",
      "\n",
      "avg / total       0.22      0.47      0.30        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svm_prediction = svm.predict(modeldata_prediction)\n",
    "print(svm_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],svm_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],svm_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on training set: 0.50\n",
      "[[ 627  208  120  130   96]\n",
      " [ 209 1173  261  207   82]\n",
      " [ 186  419 1003  188   78]\n",
      " [ 198  400  414  922   93]\n",
      " [ 219  226  202  239  387]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.44      0.53      0.48      1181\n",
      "         -1       0.48      0.61      0.54      1932\n",
      "          0       0.50      0.54      0.52      1874\n",
      "          1       0.55      0.45      0.50      2027\n",
      "          2       0.53      0.30      0.39      1273\n",
      "\n",
      "avg / total       0.50      0.50      0.49      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=1, p=2, metric='minkowski', metric_params=None, n_jobs=1)\n",
    "knn.fit(X,Y)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X,Y)))\n",
    "\n",
    "pred = knn.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-NN classifier on test set: 0.48\n",
      "[[121  46  25  23  28]\n",
      " [ 49 223  68  43  20]\n",
      " [ 41  82 182  38  17]\n",
      " [ 41  71  80 182  21]\n",
      " [ 46  43  31  45  92]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.41      0.50      0.45       243\n",
      "         -1       0.48      0.55      0.51       403\n",
      "          0       0.47      0.51      0.49       360\n",
      "          1       0.55      0.46      0.50       395\n",
      "          2       0.52      0.36      0.42       257\n",
      "\n",
      "avg / total       0.49      0.48      0.48      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = knn.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test,Y_test)))\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      "[[0 0 0 1]\n",
      " [0 0 3 2]\n",
      " [0 0 2 2]\n",
      " [0 0 4 5]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.22      0.50      0.31         4\n",
      "          1       0.50      0.56      0.53         9\n",
      "\n",
      "avg / total       0.28      0.37      0.31        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "knn_prediction = knn.predict(modeldata_prediction)\n",
    "print(knn_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],knn_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],knn_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DECISSION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.29\n",
      "[[ 131    3   77  789  181]\n",
      " [  63   63  383 1288  135]\n",
      " [  47   53  539 1154   81]\n",
      " [  47   43  382 1445  110]\n",
      " [ 101   11  113  822  226]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.34      0.11      0.17      1181\n",
      "         -1       0.36      0.03      0.06      1932\n",
      "          0       0.36      0.29      0.32      1874\n",
      "          1       0.26      0.71      0.38      2027\n",
      "          2       0.31      0.18      0.23      1273\n",
      "\n",
      "avg / total       0.33      0.29      0.24      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=2, min_samples_leaf=2, min_weight_fraction_leaf=0.013).fit(X,Y)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X,Y)))\n",
    "\n",
    "pred = clf.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on test set: 0.29\n",
      "[[ 35   1  18 159  30]\n",
      " [ 11  15  87 260  30]\n",
      " [  9  10  92 225  24]\n",
      " [ 14   9  63 281  28]\n",
      " [ 26   1  18 161  51]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.37      0.14      0.21       243\n",
      "         -1       0.42      0.04      0.07       403\n",
      "          0       0.33      0.26      0.29       360\n",
      "          1       0.26      0.71      0.38       395\n",
      "          2       0.31      0.20      0.24       257\n",
      "\n",
      "avg / total       0.34      0.29      0.24      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = clf.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test,Y_test)))\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1]\n",
      "[[0 0 0 1]\n",
      " [0 0 1 4]\n",
      " [0 0 2 2]\n",
      " [0 0 4 5]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.29      0.50      0.36         4\n",
      "          1       0.42      0.56      0.48         9\n",
      "\n",
      "avg / total       0.26      0.37      0.30        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf_prediction = clf.predict(modeldata_prediction)\n",
    "print(clf_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],clf_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],clf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05167671, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.02377403, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01705524, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02611206, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06228938, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03167737,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.76977792,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.01763729,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RANDOM FOREST CLASSIFIER on training set: 1.00\n",
      "[[1181    0    0    0    0]\n",
      " [   0 1932    0    0    0]\n",
      " [   0    0 1874    0    0]\n",
      " [   0    0    0 2027    0]\n",
      " [   0    0    0    0 1273]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, criterion='gini',max_features=6).fit(X,Y)\n",
    "print('Accuracy of RANDOM FOREST CLASSIFIER on training set: {:.2f}'\n",
    "     .format(rf.score(X,Y)))\n",
    "\n",
    "pred = rf.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RANDOM FOREST CLASSIFIER on test set: 1.00\n",
      "[[243   0   0   0   0]\n",
      " [  0 403   0   0   0]\n",
      " [  0   0 360   0   0]\n",
      " [  0   0   0 395   0]\n",
      " [  0   0   0   0 257]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       1.00      1.00      1.00       243\n",
      "         -1       1.00      1.00      1.00       403\n",
      "          0       1.00      1.00      1.00       360\n",
      "          1       1.00      1.00      1.00       395\n",
      "          2       1.00      1.00      1.00       257\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = rf.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print('Accuracy of RANDOM FOREST CLASSIFIER on test set: {:.2f}'\n",
    "     .format(rf.score(X_test,Y_test)))\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 1 1 3]\n",
      " [0 3 0 1]\n",
      " [0 3 4 2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.12      0.20      0.15         5\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       0.33      0.22      0.27         9\n",
      "\n",
      "avg / total       0.19      0.16      0.17        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf_prediction = rf.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],rf_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],rf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1551828164595149 {'criterion': 'entropy', 'max_features': 5, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "rfc = RandomForestClassifier(n_jobs=-1) \n",
    " \n",
    "# Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [50,100,200],\n",
    "           \"max_features\" : [3,4,5,6],\n",
    "           \"criterion\" : ['gini', 'entropy']}\n",
    " \n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
    "CV_rfc.fit(X=X, y=Y)\n",
    "print (CV_rfc.best_score_, CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.15301073971280318 {'criterion': 'gini', 'max_features': 5, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1181    0    0    0    0]\n",
      " [   0 1932    0    0    0]\n",
      " [   0    0 1874    0    0]\n",
      " [   0    0    0 2027    0]\n",
      " [   0    0    0    0 1273]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       1.00      1.00      1.00      1181\n",
      "         -1       1.00      1.00      1.00      1932\n",
      "          0       1.00      1.00      1.00      1874\n",
      "          1       1.00      1.00      1.00      2027\n",
      "          2       1.00      1.00      1.00      1273\n",
      "\n",
      "avg / total       1.00      1.00      1.00      8287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = CV_rfc.predict(X)\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[243   0   0   0   0]\n",
      " [  0 403   0   0   0]\n",
      " [  0   0 360   0   0]\n",
      " [  0   0   0 395   0]\n",
      " [  0   0   0   0 257]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       1.00      1.00      1.00       243\n",
      "         -1       1.00      1.00      1.00       403\n",
      "          0       1.00      1.00      1.00       360\n",
      "          1       1.00      1.00      1.00       395\n",
      "          2       1.00      1.00      1.00       257\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_model = CV_rfc.best_estimator_\n",
    "pred1 = rfc_model.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(Y_test,pred1)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y_test,pred1), recall_score(Y_test,pred1,average='macro'), precision_score(Y_test,pred1, average='macro'))\n",
    "print(classification_report(Y_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 2 3 0]\n",
      " [0 3 0 1]\n",
      " [0 3 2 4]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.22      0.40      0.29         5\n",
      "          0       0.00      0.00      0.00         4\n",
      "          1       0.80      0.44      0.57         9\n",
      "\n",
      "avg / total       0.44      0.32      0.35        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf_prediction = rfc_model.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],rf_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],rf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1,  1, -1, -1, -1, -1,  0, -1,  0,  1,  1,  0,  0,  0,\n",
       "       -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00494884, 0.00451166, 0.0047613 , 0.00471448, 0.00724065,\n",
       "       0.00763553, 0.004934  , 0.00448139, 0.00495681, 0.00457896,\n",
       "       0.00446784, 0.00461575, 0.00437668, 0.00447473, 0.00438128,\n",
       "       0.0042654 , 0.00421434, 0.00446654, 0.00470164, 0.00446825,\n",
       "       0.00427539, 0.00425216, 0.00415843, 0.0043803 , 0.00422226,\n",
       "       0.00460127, 0.00455329, 0.00344578, 0.00312384, 0.00273126,\n",
       "       0.00258246, 0.00211687, 0.0050525 , 0.00557793, 0.00433289,\n",
       "       0.00455065, 0.00421616, 0.00451405, 0.00485386, 0.00465952,\n",
       "       0.00454093, 0.0042491 , 0.00421859, 0.00426778, 0.00469227,\n",
       "       0.00624106, 0.00622273, 0.00432803, 0.00403141, 0.0049072 ,\n",
       "       0.00476016, 0.00474321, 0.00485409, 0.00531098, 0.0050026 ,\n",
       "       0.00543501, 0.00513281, 0.00430739, 0.00434956, 0.00373618,\n",
       "       0.00348301, 0.00302344, 0.0086432 , 0.00674051, 0.00631948,\n",
       "       0.00584175, 0.00558391, 0.00571141, 0.00600495, 0.00555962,\n",
       "       0.00572776, 0.00589425, 0.00628506, 0.00624871, 0.00589609,\n",
       "       0.00606082, 0.00611696, 0.00644489, 0.00589394, 0.00601109,\n",
       "       0.00651995, 0.00565316, 0.00603669, 0.00617413, 0.00667492,\n",
       "       0.00632743, 0.00651974, 0.00656684, 0.00642668, 0.00642907,\n",
       "       0.00617864, 0.00589161, 0.00603659, 0.00602058, 0.00615167,\n",
       "       0.00611878, 0.0062993 , 0.00563625, 0.00547153, 0.00629496,\n",
       "       0.00600751, 0.00617419, 0.00589708, 0.00630242, 0.00644307,\n",
       "       0.00585744, 0.00605555, 0.00619773, 0.00606179, 0.00635201,\n",
       "       0.00619629, 0.00583648, 0.00539644, 0.00522862, 0.00661882,\n",
       "       0.00693472, 0.0065511 , 0.00626598, 0.00635556, 0.00598498,\n",
       "       0.00628101, 0.00654427, 0.00627978, 0.00658945, 0.00630915,\n",
       "       0.00640461, 0.00645122, 0.00650794, 0.00677853, 0.00647781,\n",
       "       0.00638776, 0.00616178, 0.00633982, 0.00682593, 0.00663742,\n",
       "       0.00669228, 0.00636631, 0.00647437, 0.00685988, 0.0060915 ,\n",
       "       0.00597437, 0.00575054, 0.00576453, 0.00694863, 0.00719416,\n",
       "       0.00789007, 0.00695464, 0.00489535, 0.00642332, 0.00654923,\n",
       "       0.00594661, 0.00585448, 0.00536351, 0.00688227, 0.00682459,\n",
       "       0.00645674, 0.0062071 , 0.00591066, 0.00612023, 0.00673827,\n",
       "       0.00595216, 0.00586143, 0.00546304, 0.00526929, 0.00772031,\n",
       "       0.00804674, 0.00768441, 0.00764305, 0.0062697 , 0.00786809,\n",
       "       0.00588473, 0.0067684 , 0.00613286, 0.004788  , 0.00479423,\n",
       "       0.00485097, 0.00452727])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " rfc_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feature_importances = pd.DataFrame(rfc_model.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOP</th>\n",
       "      <td>0.864320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATR_30</th>\n",
       "      <td>0.804674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_fast_k</th>\n",
       "      <td>0.789007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TR</th>\n",
       "      <td>0.786809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATR_15</th>\n",
       "      <td>0.772031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATR_50</th>\n",
       "      <td>0.768441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATR_100</th>\n",
       "      <td>0.764305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Previous Day Open Interest</th>\n",
       "      <td>0.763553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.724065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_slow_d</th>\n",
       "      <td>0.719416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_fast_d</th>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoch_slow_k</th>\n",
       "      <td>0.694863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC_10</th>\n",
       "      <td>0.693472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ult_osc</th>\n",
       "      <td>0.688227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCR100_200</th>\n",
       "      <td>0.685988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCR100_10</th>\n",
       "      <td>0.682593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Willr_15</th>\n",
       "      <td>0.682459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCR_15</th>\n",
       "      <td>0.677853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chaikinAD_OSC</th>\n",
       "      <td>0.676840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCI_15</th>\n",
       "      <td>0.674051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATR_15</th>\n",
       "      <td>0.673827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCR100_30</th>\n",
       "      <td>0.669228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFI_15</th>\n",
       "      <td>0.667492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCR100_15</th>\n",
       "      <td>0.663742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPO</th>\n",
       "      <td>0.661882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCP_30</th>\n",
       "      <td>0.658945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MFI_100</th>\n",
       "      <td>0.656684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC_15</th>\n",
       "      <td>0.655110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trix_30</th>\n",
       "      <td>0.654923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROCP_10</th>\n",
       "      <td>0.654427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dema_200</th>\n",
       "      <td>0.447473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>htline</th>\n",
       "      <td>0.446825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dema_30</th>\n",
       "      <td>0.446784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_100</th>\n",
       "      <td>0.446654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_15</th>\n",
       "      <td>0.438128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kama_100</th>\n",
       "      <td>0.438030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dema_100</th>\n",
       "      <td>0.437668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROONOSC_30</th>\n",
       "      <td>0.434956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sma_15</th>\n",
       "      <td>0.433289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROON_15_down</th>\n",
       "      <td>0.432803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROONOSC_15</th>\n",
       "      <td>0.430739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kama_15</th>\n",
       "      <td>0.427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wma_100</th>\n",
       "      <td>0.426778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_30</th>\n",
       "      <td>0.426540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kama_30</th>\n",
       "      <td>0.425216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wma_30</th>\n",
       "      <td>0.424910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kama_200</th>\n",
       "      <td>0.422226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wma_50</th>\n",
       "      <td>0.421859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sma_50</th>\n",
       "      <td>0.421616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_50</th>\n",
       "      <td>0.421434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kama_50</th>\n",
       "      <td>0.415843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROON_15_up</th>\n",
       "      <td>0.403141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROONOSC_50</th>\n",
       "      <td>0.373618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROONOSC_100</th>\n",
       "      <td>0.348301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midpoint_15</th>\n",
       "      <td>0.344578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midpoint_30</th>\n",
       "      <td>0.312384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AROONOSC_200</th>\n",
       "      <td>0.302344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midpoint_50</th>\n",
       "      <td>0.273126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midpoint_100</th>\n",
       "      <td>0.258246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>midpoint_200</th>\n",
       "      <td>0.211687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            importance\n",
       "BOP                           0.864320\n",
       "NATR_30                       0.804674\n",
       "stoch_fast_k                  0.789007\n",
       "TR                            0.786809\n",
       "NATR_15                       0.772031\n",
       "NATR_50                       0.768441\n",
       "NATR_100                      0.764305\n",
       "Previous Day Open Interest    0.763553\n",
       "Volume                        0.724065\n",
       "stoch_slow_d                  0.719416\n",
       "stoch_fast_d                  0.695464\n",
       "stoch_slow_k                  0.694863\n",
       "ROC_10                        0.693472\n",
       "ult_osc                       0.688227\n",
       "ROCR100_200                   0.685988\n",
       "ROCR100_10                    0.682593\n",
       "Willr_15                      0.682459\n",
       "ROCR_15                       0.677853\n",
       "chaikinAD_OSC                 0.676840\n",
       "CCI_15                        0.674051\n",
       "ATR_15                        0.673827\n",
       "ROCR100_30                    0.669228\n",
       "MFI_15                        0.667492\n",
       "ROCR100_15                    0.663742\n",
       "PPO                           0.661882\n",
       "ROCP_30                       0.658945\n",
       "MFI_100                       0.656684\n",
       "ROC_15                        0.655110\n",
       "trix_30                       0.654923\n",
       "ROCP_10                       0.654427\n",
       "...                                ...\n",
       "dema_200                      0.447473\n",
       "htline                        0.446825\n",
       "dema_30                       0.446784\n",
       "ema_100                       0.446654\n",
       "ema_15                        0.438128\n",
       "kama_100                      0.438030\n",
       "dema_100                      0.437668\n",
       "AROONOSC_30                   0.434956\n",
       "sma_15                        0.433289\n",
       "AROON_15_down                 0.432803\n",
       "AROONOSC_15                   0.430739\n",
       "kama_15                       0.427539\n",
       "wma_100                       0.426778\n",
       "ema_30                        0.426540\n",
       "kama_30                       0.425216\n",
       "wma_30                        0.424910\n",
       "kama_200                      0.422226\n",
       "wma_50                        0.421859\n",
       "sma_50                        0.421616\n",
       "ema_50                        0.421434\n",
       "kama_50                       0.415843\n",
       "AROON_15_up                   0.403141\n",
       "AROONOSC_50                   0.373618\n",
       "AROONOSC_100                  0.348301\n",
       "midpoint_15                   0.344578\n",
       "midpoint_30                   0.312384\n",
       "AROONOSC_200                  0.302344\n",
       "midpoint_50                   0.273126\n",
       "midpoint_100                  0.258246\n",
       "midpoint_200                  0.211687\n",
       "\n",
       "[177 rows x 1 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\github\\\\PricePrediction'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importances.to_csv('D:\\\\internship\\\\Intern_AERO_40_41\\importantfeature_indicators.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG BOOST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_model = XGBClassifier()\n",
    "xg_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "print(xg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBOOST classifier on training set: 0.46\n",
      "[[ 324  216  137  391  113]\n",
      " [  63  841  403  529   96]\n",
      " [  56  236 1000  505   77]\n",
      " [  36  232  420 1247   92]\n",
      " [  65  185  139  477  407]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.60      0.27      0.38      1181\n",
      "         -1       0.49      0.44      0.46      1932\n",
      "          0       0.48      0.53      0.50      1874\n",
      "          1       0.40      0.62      0.48      2027\n",
      "          2       0.52      0.32      0.40      1273\n",
      "\n",
      "avg / total       0.48      0.46      0.45      8287\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "pred = xg_model.predict(X)\n",
    "print('Accuracy of XGBOOST classifier on training set: {:.2f}'\n",
    "     .format(xg_model.score(X,Y)))\n",
    "\n",
    "\n",
    "prediction = confusion_matrix(Y,pred)\n",
    "print(prediction)\n",
    "\n",
    "#print(accuracy_score(Y,pred), recall_score(Y,pred,average='macro'), precision_score(Y,pred, average='macro'))\n",
    "print(classification_report(Y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = xg_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.92%\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1]\n",
      " [0 0 4 1]\n",
      " [0 0 3 1]\n",
      " [0 4 4 1]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -2       0.00      0.00      0.00         1\n",
      "         -1       0.00      0.00      0.00         5\n",
      "          0       0.27      0.75      0.40         4\n",
      "          1       0.25      0.11      0.15         9\n",
      "\n",
      "avg / total       0.18      0.21      0.16        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\vishali\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "Xgboost_prediction = xg_model.predict(modeldata_prediction)\n",
    "print(confusion_matrix(modeldata_prediction_percentages['Target'],Xgboost_prediction))\n",
    "print(classification_report(modeldata_prediction_percentages['Target'],Xgboost_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
